

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.Storage &mdash; PyTorch 0.1.6 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />
  
    <link rel="stylesheet" href="_static/css/pytorch_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="PyTorch 0.1.6 documentation" href="index.html"/>
        <link rel="next" title="torch.nn" href="nn.html"/>
        <link rel="prev" title="torch.Tensor" href="tensors.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pytorch-logo-dark.svg" class="logo" />
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#excluding-subgraphs-from-backward">Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#requires-grad"><code class="docutils literal"><span class="pre">requires_grad</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/autograd.html#volatile"><code class="docutils literal"><span class="pre">volatile</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/cuda.html#best-practices">Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-autograd">Extending <code class="docutils literal"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#extending-torch-nn">Extending <code class="docutils literal"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/extending.html#adding-a-module">Adding a <code class="docutils literal"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notes/extending.html#writing-custom-c-extensions">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="notes/multiprocessing.html#best-practices-and-tips">Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild">Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torch.html#tensors">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#random-sampling">Random sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="torch.html#math-operations">Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nn.html#containers">Containers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#convolution-layers">Convolution Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#pooling-layers">Pooling Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#non-linear-activations">Non-linear Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#normalization-layers">Normalization layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#recurrent-layers">Recurrent layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#linear-layers">Linear layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#dropout-layers">Dropout layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#sparse-layers">Sparse layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#loss-functions">Loss functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#vision-layers">Vision layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html#multi-gpu-layers">Multi-GPU layers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="optim.html#how-to-use-an-optimizer">How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="optim.html#taking-an-optimization-step">Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step"><code class="docutils literal"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="optim.html#optimizer-step-closure"><code class="docutils literal"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optim.html#algorithms">Algorithms</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#variable">Variable</a><ul>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#api-compatibility">API compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-operations-on-variables">In-place operations on Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="autograd.html#function"><span class="hidden-section">Function</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="multiprocessing.html">torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="multiprocessing.html#sharing-strategies">Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="multiprocessing.html#file-system-file-system">File system - <code class="docutils literal"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="legacy.html">torch.legacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html#streams-and-events">Streams and events</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision/torchvision.html">torchvision</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchvision/datasets.html">torchvision.datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchvision/datasets.html#coco">COCO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#captions">Captions:</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/datasets.html#detection">Detection:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/datasets.html#cifar">CIFAR</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="torchvision/models.html">torchvision.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchvision/transforms.html">torchvision.transforms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html#transforms-compose"><code class="docutils literal"><span class="pre">transforms.Compose</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL.Image</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html#scale-size-interpolation-image-bilinear"><code class="docutils literal"><span class="pre">Scale(size,</span> <span class="pre">interpolation=Image.BILINEAR)</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html#centercrop-size-center-crops-the-image-to-the-given-size"><code class="docutils literal"><span class="pre">CenterCrop(size)</span></code> - center-crops the image to the given size</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html#randomcrop-size-padding-0"><code class="docutils literal"><span class="pre">RandomCrop(size,</span> <span class="pre">padding=0)</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html#randomhorizontalflip"><code class="docutils literal"><span class="pre">RandomHorizontalFlip()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html#randomsizedcrop-size-interpolation-image-bilinear"><code class="docutils literal"><span class="pre">RandomSizedCrop(size,</span> <span class="pre">interpolation=Image.BILINEAR)</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html#pad-padding-fill-0"><code class="docutils literal"><span class="pre">Pad(padding,</span> <span class="pre">fill=0)</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html#normalize-mean-std"><code class="docutils literal"><span class="pre">Normalize(mean,</span> <span class="pre">std)</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="torchvision/transforms.html#generic-transofrms">Generic Transofrms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/transforms.html#lambda-lambda"><code class="docutils literal"><span class="pre">Lambda(lambda)</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="torchvision/utils.html">torchvision.utils</a><ul>
<li class="toctree-l2"><a class="reference internal" href="torchvision/utils.html#make-grid-tensor-nrow-8-padding-2">make_grid(tensor, nrow=8, padding=2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="torchvision/utils.html#save-image-tensor-filename-nrow-8-padding-2">save_image(tensor, filename, nrow=8, padding=2)</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">PyTorch</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>torch.Storage</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/storage.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="torch-storage">
<h1>torch.Storage<a class="headerlink" href="#torch-storage" title="Permalink to this headline">¶</a></h1>
<p>A <code class="xref py py-class docutils literal"><span class="pre">torch.Storage</span></code> is a contiguous, one-dimensional array of a single
data type.</p>
<p>Every <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><code class="xref py py-class docutils literal"><span class="pre">torch.Tensor</span></code></a> has a corresponding storage of the same data type.</p>
<dl class="class">
<dt id="torch.FloatStorage">
<em class="property">class </em><code class="descclassname">torch.</code><code class="descname">FloatStorage</code><a class="reference internal" href="_modules/torch.html#FloatStorage"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.FloatStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="torch.FloatStorage.byte">
<code class="descname">byte</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.byte" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to byte type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.char">
<code class="descname">char</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.char" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to char type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.clone">
<code class="descname">clone</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.clone" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.copy_">
<code class="descname">copy_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.copy_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.cpu">
<code class="descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a CPU copy of this storage if it&#8217;s not already on the CPU</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.cuda">
<code class="descname">cuda</code><span class="sig-paren">(</span><em>device=None</em>, <em>async=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a copy of this object in CUDA memory.</p>
<p>If this object is already in CUDA memory and on the correct device, then
no copy is performed and the original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>device</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) &#8211; The destination GPU id. Defaults to the current device.</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True and the source is in pinned memory, the copy will
be asynchronous with respect to the host. Otherwise, the
argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.data_ptr">
<code class="descname">data_ptr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.data_ptr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.double">
<code class="descname">double</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to double type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.element_size">
<code class="descname">element_size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.element_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.fill_">
<code class="descname">fill_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.fill_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.float">
<code class="descname">float</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to float type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.from_buffer">
<code class="descname">from_buffer</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.from_buffer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.half">
<code class="descname">half</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to half type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.int">
<code class="descname">int</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.int" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to int type</p>
</dd></dl>

<dl class="attribute">
<dt id="torch.FloatStorage.is_cuda">
<code class="descname">is_cuda</code><em class="property"> = False</em><a class="headerlink" href="#torch.FloatStorage.is_cuda" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.is_pinned">
<code class="descname">is_pinned</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.is_pinned" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.is_shared">
<code class="descname">is_shared</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.is_shared" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.long">
<code class="descname">long</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.long" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to long type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.new">
<code class="descname">new</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.new" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.pin_memory">
<code class="descname">pin_memory</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.pin_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies the storage to pinned memory, if it&#8217;s not already pinned.</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.resize_">
<code class="descname">resize_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.resize_" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.share_memory_">
<code class="descname">share_memory_</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.share_memory_" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the storage to shared memory.</p>
<p>This is a no-op for storages already in shared memory and for CUDA
storages, which do not need to be moved for sharing across processes.
Storages in shared memory cannot be resized.</p>
<p>Returns: self</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.short">
<code class="descname">short</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.short" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this storage to short type</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.size">
<code class="descname">size</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.tolist">
<code class="descname">tolist</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.tolist" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list containing the elements of this storage</p>
</dd></dl>

<dl class="method">
<dt id="torch.FloatStorage.type">
<code class="descname">type</code><span class="sig-paren">(</span><em>new_type=None</em>, <em>async=False</em><span class="sig-paren">)</span><a class="headerlink" href="#torch.FloatStorage.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts this object to the specified type.</p>
<p>If this is already of the correct type, no copy is performed and the
original object is returned.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>new_type</strong> (<a class="reference internal" href="tensors.html#torch.Tensor.type" title="torch.Tensor.type"><em>type</em></a><em> or </em><a class="reference external" href="https://docs.python.org/2/library/string.html#module-string" title="(in Python v2.7)"><em>string</em></a>) &#8211; The desired type</li>
<li><strong>async</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) &#8211; If True, and the source is in pinned memory and
destination is on the GPU or vice versa, the copy is
performed asynchronously with respect to the host.
Otherwise, the argument has no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="nn.html" class="btn btn-neutral float-right" title="torch.nn" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tensors.html" class="btn btn-neutral" title="torch.Tensor" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.6',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>