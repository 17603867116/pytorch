<h1 id="torch-optim">torch.optim</h1>

<p>The Optim package in Torch is targeted for one to optimize their neural networks
using a wide variety of optimization methods such as SGD, Adam etc.</p>

<p>Currently, the following optimization methods are supported, typically with
options such as weight decay and other bells and whistles.</p>

<ul>
<li>SGD          <code class="prettyprint">(params, lr=required, momentum=0, dampening=0)</code></li>
<li>AdaDelta     <code class="prettyprint">(params, rho=0.9, eps=1e-6, weight_decay=0)</code></li>
<li>Adagrad      <code class="prettyprint">(params, lr=1e-2, lr_decay=0, weight_decay=0)</code></li>
<li>Adam         <code class="prettyprint">(params, lr=1e-2, betas=(0.9, 0.999), epsilon=1e-8, weight_decay=0)</code></li>
<li>AdaMax       <code class="prettyprint">(params, lr=1e-2, betas=(0.9, 0.999), eps=1e-38, weight_decay=0)</code></li>
<li>Averaged SGD <code class="prettyprint">(params, lr=1e-2, lambd=1e-4, alpha=0.75, t0=1e6, weight_decay=0)</code></li>
<li>RProp        <code class="prettyprint">(params, lr=1e-2, etas=(0.5, 1.2), step_sizes=(1e-6, 50))</code></li>
<li>RMSProp      <code class="prettyprint">(params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0)</code></li>
</ul>

<p>The usage of the Optim package itself is as follows.</p>

<ol>
<li>Construct an optimizer</li>
<li>Use <code class="prettyprint">optimizer.step(...)</code> to optimize.

<ul>
<li>Call <code class="prettyprint">optimizer.zero_grad()</code> to zero out the gradient buffers when appropriate</li>
</ul></li>
</ol>

<h2 id="1-constructing-the-optimizer">1. Constructing the optimizer</h2>

<p>One first constructs an <code class="prettyprint">Optimizer</code> object by giving it a list of parameters
to optimize, as well as the optimizer options,such as learning rate, weight decay, etc.</p>

<p>Examples:</p>

<p><code class="prettyprint">optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)</code></p>

<p><code class="prettyprint">optimizer = optim.Adam([var1, var2], lr = 0.0001)</code></p>

<h3 id="per-parameter-options">Per-parameter options</h3>

<p>In a more advanced usage, one can specify per-layer options by passing each parameter group along with it&rsquo;s custom options.</p>

<p><strong><strong>Any parameter group that does not have an attribute defined will use the default attributes.</strong></strong></p>

<p>This is very useful when one wants to specify per-layer learning rates for example.</p>

<p>Example:</p>

<p><code class="prettyprint">optim.SGD([{&#39;params&#39;: model1.parameters()}, {&#39;params&#39;: model2.parameters(), &#39;lr&#39;: 1e-3}, lr=1e-2, momentum=0.9)</code></p>

<p><code class="prettyprint">model1</code>&rsquo;s parameters will use the default learning rate of <code class="prettyprint">1e-2</code> and momentum of <code class="prettyprint">0.9</code>
<code class="prettyprint">model2</code>&rsquo;s parameters will use a learning rate of <code class="prettyprint">1e-3</code>, and the default momentum of <code class="prettyprint">0.9</code></p>

<p>Then, you can use the optimizer by calling <code class="prettyprint">optimizer.zero_grad()</code> and <code class="prettyprint">optimizer.step(...)</code>. Read the next sections.</p>

<h2 id="2-taking-an-optimization-step-using-optimizer-step">2. Taking an optimization step using <code class="prettyprint">Optimizer.step(...)</code></h2>

<p>The step function has the following two signatures:</p>

<h3 id="a-optimizer-step-closure">a. <code class="prettyprint">Optimizer.step(closure)</code></h3>

<p>The <code class="prettyprint">step</code> function takes a user-defined closure that computes f(x) and returns the loss.</p>

<p>The closure needs to do the following:
- Optimizer.zero_grad()
- Compute the loss
- Call loss.backward()
- return the loss</p>

<p>Example 1: training a neural network</p>
<pre class="highlight python"><code><span class="c"># Example 1: training a neural network with optimizer.step(closure)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MNISTNet</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">ClassNLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_batches</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
</code></pre>

<p>Notes: Why is this required? Why cant we simply have the optimizer take the parameters and grads?
       Some optimization algorithms such as Conjugate Gradient and LBFGS need to evaluate their function
       multiple times. For such optimization methods, the function (i.e. the closure) has to be defined.</p>

<h3 id="b-optimizer-step">b. <code class="prettyprint">Optimizer.step()</code></h3>

<p>This is a simplified usage that supports most, but not all optimization algorithms. For example, it does not support LBFGS or Conjugate Gradient.</p>

<p>The usage for this is to simply call the function after the backward() is called on your model.</p>

<p>Example 2: training a neural network</p>
<pre class="highlight python"><code><span class="c"># Example 2: training a neural network with optimizer.step()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MNISTNet</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">ClassNLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_batches</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre>
