<h2 id="loss-functions">Loss functions</h2>

<h3 id="l1loss">L1Loss</h3>

<p>Creates a criterion that measures the mean absolute value of the </p>

<p>element-wise difference between input <code class="prettyprint">x</code> and target <code class="prettyprint">y</code>:</p>

<p>loss(x, y)  = 1/n \sum |x_i - y_i|</p>

<p><code class="prettyprint">x</code> and <code class="prettyprint">y</code> arbitrary shapes with a total of <code class="prettyprint">n</code> elements each
the sum operation still operates over all the elements, and divides by <code class="prettyprint">n</code>.</p>

<p>The division by <code class="prettyprint">n</code> can be avoided if one sets the internal 
variable <code class="prettyprint">sizeAverage</code> to <code class="prettyprint">False</code></p>

<h3 id="mseloss">MSELoss</h3>

<p>Creates a criterion that measures the mean squared error between </p>

<p><code class="prettyprint">n</code> elements in the input <code class="prettyprint">x</code> and target <code class="prettyprint">y</code>:
    loss(x, y) = 1/n \sum |x_i - y_i|^2
<code class="prettyprint">x</code> and <code class="prettyprint">y</code> arbitrary shapes with a total of <code class="prettyprint">n</code> elements each
the sum operation still operates over all the elements, and divides by <code class="prettyprint">n</code>.</p>

<p>The division by <code class="prettyprint">n</code> can be avoided if one sets the internal variable 
<code class="prettyprint">sizeAverage</code> to <code class="prettyprint">False</code>
By default, the losses are averaged over observations for each minibatch. 
However, if the field <code class="prettyprint">sizeAverage = False</code>, the losses are instead summed.</p>

<h3 id="crossentropyloss">CrossEntropyLoss</h3>

<p>This criterion combines <code class="prettyprint">LogSoftMax</code> and <code class="prettyprint">ClassNLLLoss</code> in one single class.</p>

<p>It is useful when training a classification problem with <code class="prettyprint">n</code> classes.
If provided, the optional argument <code class="prettyprint">weights</code> should be a 1D <code class="prettyprint">Tensor</code> 
assigning weight to each of the classes. 
This is particularly useful when you have an unbalanced training set.</p>

<p>The <code class="prettyprint">input</code> is expected to contain scores for each class: 
      <code class="prettyprint">input</code> has to be a 2D <code class="prettyprint">Tensor</code> of size <code class="prettyprint">batch x n</code>.
This criterion expects a class index (0 to nClasses-1) as the 
<code class="prettyprint">target</code> for each value of a 1D tensor of size <code class="prettyprint">n</code></p>

<p>The loss can be described as:</p>

<p>loss(x, class) = -log(exp(x[class]) / (\sum_j exp(x[j])))
               = -x[class] + log(\sum_j exp(x[j]))</p>

<p>or in the case of the <code class="prettyprint">weights</code> argument being specified:</p>

<p>loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))</p>

<p>The losses are averaged across observations for each minibatch.</p>

<h3 id="nllloss">NLLLoss</h3>

<p>The negative log likelihood loss. It is useful to train a classication problem with n classes</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="c"># input is of size nBatch x nClasses = 3 x 5</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="c"># each element in target has to have 0 &lt;= value &lt; nclasses </span>
<span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre>

<p>If provided, the optional argument <code class="prettyprint">weights</code> should be a 1D Tensor assigning
weight to each of the classes.
This is particularly useful when you have an unbalanced training set.</p>

<p>The input given through a forward call is expected to contain log-probabilities
of each class: input has to be a 2D Tensor of size minibatch x n
Obtaining log-probabilities in a neural network is easily achieved by
adding a  <code class="prettyprint">LogSoftmax</code>  layer in the last layer.
You may use <code class="prettyprint">CrossEntropyLoss</code>  instead, if you prefer not to
add an extra layer.</p>

<p>The target that this loss expects is a class index (1 to the number of class)</p>

<p>The loss can be described as:
    loss(x, class) = -x[class]</p>

<p>or in the case of the weights argument it is specified as follows:
    loss(x, class) = -weights[class] * x[class]</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>None</td>
<td>a manual rescaling weight given to each class. If given, has to be a Tensor of size &ldquo;nclasses&rdquo;.</td>
</tr>
<tr>
<td>size_average</td>
<td>True</td>
<td>By default, the losses are averaged over observations for each minibatch. However, if the field sizeAverage is set to False, the losses are instead summed for each minibatch.</td>
</tr>
</tbody></table>

<p>Target Shape: [ * ] : Targets of size [minibatch], each value has to be 1 &lt;= targets[i] &lt;= nClasses</p>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>the class-weights given as input to the constructor</td>
</tr>
</tbody></table>

<h3 id="nllloss2d">NLLLoss2d</h3>

<p>This is negative log likehood loss, but for image inputs. It computes NLL loss per-pixel.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss2d</span><span class="p">()</span>
<span class="c"># input is of size nBatch x nClasses x height x width</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="c"># each element in target has to have 0 &lt;= value &lt; nclasses</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre>

<p>This loss does not support per-class weights</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>size_average</td>
<td>True</td>
<td>By default, the losses are averaged over observations for each minibatch. However, if the field sizeAverage is set to False, the losses are instead summed for each minibatch.</td>
</tr>
</tbody></table>

<p>Target Shape: [ * , *, *] : Targets of size minibatch x height x width, each value has to be 1 &lt;= targets[i] &lt;= nClasses</p>

<h3 id="kldivloss">KLDivLoss</h3>

<p>The <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a> Loss</p>

<p>KL divergence is a useful distance measure for continuous distributions 
and is often useful when performing direct regression over the space of
(discretely sampled) continuous output distributions.
As with ClassNLLLoss, the <code class="prettyprint">input</code> given is expected to contain 
<em>log-probabilities</em>, however unlike ClassNLLLoss, <code class="prettyprint">input</code> is not 
restricted to a 2D Tensor, because the criterion is applied element-wise.</p>

<p>This criterion expects a <code class="prettyprint">target</code> <code class="prettyprint">Tensor</code> of the same size as the 
<code class="prettyprint">input</code> <code class="prettyprint">Tensor</code>.</p>

<p>The loss can be described as:
    loss(x, target) = 1/n \sum(target_i * (log(target_i) - x_i))</p>

<p>By default, the losses are averaged for each minibatch over observations 
<em>as well as</em> over dimensions. However, if the field 
<code class="prettyprint">sizeAverage</code> is set to <code class="prettyprint">False</code>, the losses are instead summed.</p>

<h3 id="bceloss">BCELoss</h3>

<p>Creates a criterion that measures the Binary Cross Entropy </p>

<p>between the target and the output:
    loss(o, t) = - 1/n sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))</p>

<p>or in the case of the weights argument being specified:
    loss(o, t) = - 1/n sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))</p>

<p>This is used for measuring the error of a reconstruction in for example 
an auto-encoder. Note that the targets <code class="prettyprint">t[i]</code> should be numbers between 0 and 1, 
for instance, the output of an <code class="prettyprint">nn.Sigmoid</code> layer.</p>

<p>By default, the losses are averaged for each minibatch over observations 
<em>as well as</em> over dimensions. However, if the field <code class="prettyprint">sizeAverage</code> is set 
to <code class="prettyprint">False</code>, the losses are instead summed.</p>

<h3 id="marginrankingloss">MarginRankingLoss</h3>

<p>Creates a criterion that measures the loss given  </p>

<p>inputs <code class="prettyprint">x1</code>, <code class="prettyprint">x2</code>, two 1D min-batch <code class="prettyprint">Tensor</code>s, 
and a label 1D mini-batch tensor <code class="prettyprint">y</code> with values (<code class="prettyprint">1</code> or <code class="prettyprint">-1</code>).</p>

<p>If <code class="prettyprint">y == 1</code> then it assumed the first input should be ranked higher 
(have a larger value) than the second input, and vice-versa for <code class="prettyprint">y == -1</code>.</p>

<p>The loss function for each sample in the mini-batch is:</p>

<p>loss(x, y) = max(0, -y * (x1 - x2) + margin)</p>

<p>if the internal variable <code class="prettyprint">sizeAverage = True</code>, 
the loss function averages the loss over the batch samples; 
if <code class="prettyprint">sizeAverage = False</code>, then the loss function sums over the batch samples. 
By default, <code class="prettyprint">sizeAverage</code> equals to <code class="prettyprint">True</code>.</p>

<h3 id="hingeembeddingloss">HingeEmbeddingLoss</h3>

<p>Measures the loss given an input <code class="prettyprint">x</code> which is a 2D mini-batch tensor</p>

<p>and a labels <code class="prettyprint">y</code>, a 1D tensor containg values (<code class="prettyprint">1</code> or <code class="prettyprint">-1</code>).
This is usually used for measuring whether two inputs are similar or dissimilar, 
e.g. using the L1 pairwise distance, and is typically used for learning 
nonlinear embeddings or semi-supervised learning.</p>

<p>{ x_i,                  if y_i ==  1
    loss(x, y) = 1/n {
                     { max(0, margin - x_i), if y_i == -1</p>

<p><code class="prettyprint">x</code> and <code class="prettyprint">y</code> arbitrary shapes with a total of <code class="prettyprint">n</code> elements each
the sum operation still operates over all the elements, and divides by <code class="prettyprint">n</code>.
(the division by <code class="prettyprint">n</code> can be avoided if one sets the internal variable <code class="prettyprint">sizeAverage=False</code>). 
The <code class="prettyprint">margin</code> has a default value of <code class="prettyprint">1</code>, or can be set in the constructor.</p>

<h3 id="multilabelmarginloss">MultiLabelMarginLoss</h3>

<p>Creates a criterion that optimizes a multi-class multi-classification </p>

<p>hinge loss (margin-based loss) between input <code class="prettyprint">x</code>  (a 2D mini-batch <code class="prettyprint">Tensor</code>) and 
output <code class="prettyprint">y</code> (which is a 2D <code class="prettyprint">Tensor</code> of target class indices).
For each sample in the mini-batch:</p>

<p>loss(x, y) = sum_ij(max(0, 1 - (x[y[j]] - x[i]))) / x:size(1)</p>

<p>where <code class="prettyprint">i == 0</code> to <code class="prettyprint">x.size(0)</code>, <code class="prettyprint">j == 0</code> to <code class="prettyprint">y.size(0)</code>, 
      <code class="prettyprint">y[j] != 0</code>, and <code class="prettyprint">i != y[j]</code> for all <code class="prettyprint">i</code> and <code class="prettyprint">j</code>.</p>

<p><code class="prettyprint">y</code> and <code class="prettyprint">x</code> must have the same size.
The criterion only considers the first non zero <code class="prettyprint">y[j]</code> targets.
This allows for different samples to have variable amounts of target classes</p>

<h3 id="smoothl1loss">SmoothL1Loss</h3>

<p>Creates a criterion that uses a squared term if the absolute </p>

<p>element-wise error falls below 1 and an L1 term otherwise. 
It is less sensitive to outliers than the <code class="prettyprint">MSELoss</code> and in some cases 
prevents exploding gradients (e.g. see &ldquo;Fast R-CNN&rdquo; paper by Ross Girshick).
Also known as the Huber loss.</p>

<p>{ 0.5 * (x_i - y_i)^2, if |x_i - y_i| &lt; 1
    loss(x, y) = 1/n \sum {
                          { |x_i - y_i| - 0.5,   otherwise</p>

<p><code class="prettyprint">x</code> and <code class="prettyprint">y</code> arbitrary shapes with a total of <code class="prettyprint">n</code> elements each
the sum operation still operates over all the elements, and divides by <code class="prettyprint">n</code>.</p>

<p>The division by <code class="prettyprint">n</code> can be avoided if one sets the internal variable 
<code class="prettyprint">sizeAverage</code> to <code class="prettyprint">False</code></p>

<h3 id="softmarginloss">SoftMarginLoss</h3>

<p>Creates a criterion that optimizes a two-class classification </p>

<p>logistic loss between input <code class="prettyprint">x</code> (a 2D mini-batch <code class="prettyprint">Tensor</code>) and 
target <code class="prettyprint">y</code> (which is a tensor containing either <code class="prettyprint">1</code>s or <code class="prettyprint">-1</code>s).</p>

<p>loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x:nElement()</p>

<p>The normalization by the number of elements in the input can be disabled by
setting <code class="prettyprint">self.sizeAverage</code> to <code class="prettyprint">False</code>.</p>

<h3 id="multilabelsoftmarginloss">MultiLabelSoftMarginLoss</h3>

<p>Creates a criterion that optimizes a multi-label one-versus-all </p>

<p>loss based on max-entropy, between input <code class="prettyprint">x</code>  (a 2D mini-batch <code class="prettyprint">Tensor</code>) and 
target <code class="prettyprint">y</code> (a binary 2D <code class="prettyprint">Tensor</code>). For each sample in the minibatch:</p>

<p>loss(x, y) = - sum_i (y[i] log( exp(x[i]) / (1 + exp(x[i]))) 
                         + (1-y[i]) log(1/(1+exp(x[i])))) / x:nElement()</p>

<p>where <code class="prettyprint">i == 0</code> to <code class="prettyprint">x.nElement()-1</code>, <code class="prettyprint">y[i]  in {0,1}</code>.
<code class="prettyprint">y</code> and <code class="prettyprint">x</code> must have the same size.</p>

<h3 id="cosineembeddingloss">CosineEmbeddingLoss</h3>

<p>Creates a criterion that measures the loss given  an input tensors x1, x2 </p>

<p>and a <code class="prettyprint">Tensor</code> label <code class="prettyprint">y</code> with values 1 or -1.
This is used for measuring whether two inputs are similar or dissimilar, 
using the cosine distance, and is typically used for learning nonlinear 
embeddings or semi-supervised learning.</p>

<p><code class="prettyprint">margin</code> should be a number from <code class="prettyprint">-1</code> to <code class="prettyprint">1</code>, <code class="prettyprint">0</code> to <code class="prettyprint">0.5</code> is suggested.
If <code class="prettyprint">margin</code> is missing, the default value is <code class="prettyprint">0</code>.</p>

<p>The loss function for each sample is:</p>

<p>{ 1 - cos(x1, x2),              if y ==  1
    loss(x, y) = {
                 { max(0, cos(x1, x2) - margin), if y == -1</p>

<p>If the internal variable <code class="prettyprint">sizeAverage</code> is equal to <code class="prettyprint">True</code>, 
the loss function averages the loss over the batch samples; 
if <code class="prettyprint">sizeAverage</code> is <code class="prettyprint">False</code>, then the loss function sums over the 
batch samples. By default, <code class="prettyprint">sizeAverage = True</code>.</p>

<h3 id="multimarginloss">MultiMarginLoss</h3>

<p>Creates a criterion that optimizes a multi-class classification hinge loss </p>

<p>(margin-based loss) between input <code class="prettyprint">x</code> (a 2D mini-batch <code class="prettyprint">Tensor</code>) and 
output <code class="prettyprint">y</code> (which is a 1D tensor of target class indices, <code class="prettyprint">0</code> &lt;= <code class="prettyprint">y</code> &lt;= <code class="prettyprint">x.size(1)</code>):</p>

<p>For each mini-batch sample:
    loss(x, y) = sum_i(max(0, (margin - x[y] + x[i]))^p) / x.size(0)
                 where <code class="prettyprint">i == 0</code> to <code class="prettyprint">x.size(0)</code> and <code class="prettyprint">i != y</code>.</p>

<p>Optionally, you can give non-equal weighting on the classes by passing 
a 1D <code class="prettyprint">weights</code> tensor into the constructor.</p>

<p>The loss function then becomes:
    loss(x, y) = sum_i(max(0, w[y] * (margin - x[y] - x[i]))^p) / x.size(0)</p>

<p>By default, the losses are averaged over observations for each minibatch. 
However, if the field <code class="prettyprint">sizeAverage</code> is set to <code class="prettyprint">False</code>, 
the losses are instead summed.</p>
