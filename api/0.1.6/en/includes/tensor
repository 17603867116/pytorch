<h1 id="tensors">Tensors</h1>

<p>A <code class="prettyprint">Tensor</code> is a potentially multi-dimensional matrix.
The number of dimensions is unlimited.</p>

<p>The <code class="prettyprint">Tensor</code> set of classes are probably the most important class in
<code class="prettyprint">torch</code>. Almost every package depends on these classes. They are <em><strong>the</strong></em>
class for handling numeric data. As with pretty much anything in
[torch], tensors are serializable with <code class="prettyprint">torch.save</code> and <code class="prettyprint">torch.load</code></p>

<p>There are 7 Tensor classes in torch:</p>

<ul>
<li><code class="prettyprint">torch.FloatTensor</code>   :   Signed 32-bit floating point tensor</li>
<li><code class="prettyprint">torch.DoubleTensor</code>  :   Signed 64-bit floating point tensor</li>
<li><code class="prettyprint">torch.ByteTensor</code>    :   Signed  8-bit integer tensor</li>
<li><code class="prettyprint">torch.CharTensor</code>    : Unsigned  8-bit integer tensor</li>
<li><code class="prettyprint">torch.ShortTensor</code>   :   Signed 16-bit integer tensor</li>
<li><code class="prettyprint">torch.IntTensor</code>     :   Signed 32-bit integer tensor</li>
<li><code class="prettyprint">torch.LongTensor</code>    :   Signed 64-bit integer tensor</li>
</ul>

<p>The data in these tensors lives on the system memory connected to your CPU.</p>

<p>Most numeric operations are implemented <em>only</em> for <code class="prettyprint">FloatTensor</code> and <code class="prettyprint">DoubleTensor</code>.
Other Tensor types are useful if you want to save memory space or specifically
do integer operations.</p>

<p>The number of dimensions of a <code class="prettyprint">Tensor</code> can be queried by
<code class="prettyprint">ndimension()</code> or <code class="prettyprint">dim()</code>. Size of the <code class="prettyprint">i-th</code> dimension is
returned by <code class="prettyprint">size(i)</code>. A tuple containing the size of all the dimensions
can be returned by <code class="prettyprint">size()</code>.</p>
<pre class="highlight python"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="c"># allocate a matrix of shape 3x4</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c"># convert this into a LongTensor</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="nb">long</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="c"># print the size of the tensor</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<span class="c"># print the number of dimensions</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span>
</code></pre>

<p>These tensors can be converted to numpy arrays very efficiently
with zero memory copies.
For this, the two provided functions are <code class="prettyprint">.numpy()</code> and <code class="prettyprint">torch.from_numpy()</code></p>
<pre class="highlight python"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c"># convert to numpy</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
</code></pre>

<p>When using GPUs, each of the classes above has an equivalent
class such as: <code class="prettyprint">torch.cuda.FloatTensor</code>, <code class="prettyprint">torch.cuda.LongTensor</code>, etc.
When one allocates a CUDA tensor, the data in these tensors lives in the
GPU memory.</p>

<p>One can seamlessly transfer a tensor from the CPU to the GPU, as well as
between different GPUs on your machine.</p>

<p>Apart from the above 7 tensor types, there is one additional tensor type on the GPU</p>

<ul>
<li><code class="prettyprint">torch.cuda.HalfTensor</code> : Signed 16-bit floating point tensor</li>
</ul>
<pre class="highlight python"><code><span class="kn">import</span> <span class="nn">torch.cuda</span>

<span class="c"># allocate a matrix of shape 3x4</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c"># transfer this to the CPU</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="c"># transfer this back to the GPU-1</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c"># transfer this to GPU-2</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre>

<h2 id="internal-data-representation">Internal data representation</h2>

<p>The actual data of a <code class="prettyprint">Tensor</code> is contained into a
<code class="prettyprint">Storage</code>. It can be accessed using
<code class="prettyprint">storage()</code>. While the memory of a
<code class="prettyprint">Tensor</code> has to be contained in this unique <code class="prettyprint">Storage</code>, it might
not be contiguous: the first position used in the <code class="prettyprint">Storage</code> is given
by <code class="prettyprint">storage_offset()</code> (starting at <code class="prettyprint">0</code>).
And the <em>jump</em> needed to go from one element to another
element in the <code class="prettyprint">i-th</code> dimension is given by
<code class="prettyprint">stride(i-1)</code>. See the code example for an illustration.</p>
<pre class="highlight python"><code><span class="c"># given a 3d tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>

<span class="c"># accessing the element `(3,4,5)` can be done by</span>
<span class="n">x</span><span class="p">[</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span class="c"># or equivalently (but slowly!)</span>
<span class="n">x</span><span class="o">.</span><span class="n">storage</span><span class="p">()[</span><span class="n">x</span><span class="o">.</span><span class="n">storageOffset</span><span class="p">()</span>
            <span class="o">+</span> <span class="p">(</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="p">(</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="o">+</span> <span class="p">(</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
</code></pre>

<p>One could say that a <code class="prettyprint">Tensor</code> is a particular way of <em>viewing</em> a
<code class="prettyprint">Storage</code>: a <code class="prettyprint">Storage</code> only represents a chunk of memory, while the
<code class="prettyprint">Tensor</code> interprets this chunk of memory as having dimensions:</p>
<pre class="highlight python"><code><span class="c"># a tensor interprets a chunk of memory as having dimensions</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span> <span class="c"># fill up the Storage</span>
<span class="o">&gt;&gt;&gt;</span>   <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

<span class="c"># s is interpreted by x as a 2D matrix</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="mi">1</span>   <span class="mi">2</span>   <span class="mi">3</span>   <span class="mi">4</span>   <span class="mi">5</span>
  <span class="mi">6</span>   <span class="mi">7</span>   <span class="mi">8</span>   <span class="mi">9</span>  <span class="mi">10</span>
 <span class="mi">11</span>  <span class="mi">12</span>  <span class="mi">13</span>  <span class="mi">14</span>  <span class="mi">15</span>
 <span class="mi">16</span>  <span class="mi">17</span>  <span class="mi">18</span>  <span class="mi">19</span>  <span class="mi">20</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">4</span><span class="n">x5</span><span class="p">]</span>
</code></pre>

<p>Note also that in Torch7 <strong><em>elements in the same row</em></strong> [elements along the <strong>last</strong> dimension]
are contiguous in memory for a matrix [tensor]:</p>

<p>This is exactly like in <code class="prettyprint">C</code> and <code class="prettyprint">numpy</code> (and not <code class="prettyprint">Fortran</code>).</p>

<h2 id="default-tensor-type">Default Tensor type</h2>

<p>For convenience, <em>an alias</em> <code class="prettyprint">torch.Tensor</code> is provided, which allows the user to write
type-independent scripts, which can then ran after choosing the desired Tensor type with
a call like</p>

<p><code class="prettyprint">torch.set_default_tensor_type(&#39;torch.DoubleTensor&#39;)</code></p>

<p>By default, the alias points to <code class="prettyprint">torch.FloatTensor</code>.</p>

<h2 id="efficient-memory-management">Efficient memory management</h2>

<p><em>All</em> tensor operations post-fixed with an underscore (for example <code class="prettyprint">.fill_</code>)
do <em>not</em> make any memory copy. All these methods transform the existing tensor.
Tensor methods such as <code class="prettyprint">narrow</code> and <code class="prettyprint">select</code> return a new tensor referencing <em>the same storage</em>.
This magical behavior is internally obtained by good usage of the <code class="prettyprint">stride()</code> and
<code class="prettyprint">storage_offset()</code>. See the code example illustrating this.</p>
<pre class="highlight python"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">5</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># narrow() returns a Tensor referencing the same Storage as x</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="mi">0</span>
 <span class="mi">1</span>
 <span class="mi">1</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">5</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># same thing can be achieved with slice indexing</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="mi">0</span>
 <span class="mi">2</span>
 <span class="mi">2</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">5</span><span class="p">]</span>
</code></pre>

<p>If you really need to copy a <code class="prettyprint">Tensor</code>, you can use the <code class="prettyprint">copy_()</code> method:</p>
<pre class="highlight python"><code><span class="c"># making a copy of a tensor</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</code></pre>

<p>Or the convenience method <code class="prettyprint">clone()</code></p>

<p>We now describe all the methods for <code class="prettyprint">Tensor</code>. If you want to specify the Tensor type,
just replace <code class="prettyprint">Tensor</code> by the name of the Tensor variant (like <code class="prettyprint">CharTensor</code>).</p>

<h2 id="constructors">Constructors</h2>

<p>Tensor constructors, create new Tensor object, optionally, allocating
new memory. By default the elements of a newly allocated memory are
not initialized, therefore, might contain arbitrary numbers. Here are
several ways to construct a new <code class="prettyprint">Tensor</code>.</p>

<h3 id="torch-tensor">torch.Tensor()</h3>

<p>Returns an empty tensor.</p>

<h3 id="torch-tensor-tensor">torch.Tensor(tensor)</h3>

<p>Returns a new tensor which reference the same <code class="prettyprint">Storage</code> than the given <code class="prettyprint">tensor</code>.
The <code class="prettyprint">size</code>, <code class="prettyprint">stride</code>, and <code class="prettyprint">storage_offset</code> are the same than the given tensor.</p>

<p>The new <code class="prettyprint">Tensor</code> is now going to &ldquo;view&rdquo; the same <code class="prettyprint">storage</code>
as the given <code class="prettyprint">tensor</code>. As a result, any modification in the elements
of the <code class="prettyprint">Tensor</code> will have a impact on the elements of the given
<code class="prettyprint">tensor</code>, and vice-versa. No memory copy!</p>
<pre class="highlight python"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">3.14</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
 <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>
 <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">2</span><span class="n">x5</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span>
 <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>
 <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">2</span><span class="n">x5</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="c"># elements of x are the same as y!</span>
<span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">2</span><span class="n">x5</span><span class="p">]</span>
</code></pre>

<h3 id="torch-tensor-sz1-sz2-sz3-sz4-sz5">torch.Tensor(sz1 [,sz2 [,sz3 [,sz4 [,sz5 &hellip;]]]]])</h3>

<p>Create a tensor of the given sizes.
The tensor size will be <code class="prettyprint">sz1 x sz2 x sx3 x sz4 x sz5 x ...</code>.</p>

<h3 id="torch-tensor-sizes">torch.Tensor(sizes)</h3>

<p>Create a tensor of any number of dimensions. <code class="prettyprint">sizes</code> gives the size in each dimension of
the tensor and is of type <code class="prettyprint">torch.Size</code>.</p>
<pre class="highlight python"><code><span class="n">Example</span><span class="p">,</span> <span class="n">create</span> <span class="n">a</span> <span class="mi">4</span><span class="n">D</span> <span class="mi">4</span><span class="n">x4x3x2</span> <span class="n">tensor</span><span class="p">:</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]))</span>
</code></pre>

<h3 id="torch-tensor-storage">torch.Tensor(storage)</h3>

<p>Returns a tensor which uses the existing <code class="prettyprint">Storage</code> starting at a storage offset of 0.</p>

<h3 id="torch-tensor-sequence">torch.Tensor(sequence)</h3>

<p>One can create a tensor from a python sequence.</p>

<p>For example, you can create a <code class="prettyprint">Tensor</code> from a <code class="prettyprint">list</code> or a <code class="prettyprint">tuple</code></p>
<pre class="highlight python"><code><span class="c"># create a 2d tensor from a list of lists</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">]])</span>
 <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>  <span class="mi">4</span>
 <span class="mi">5</span>  <span class="mi">6</span>  <span class="mi">7</span>  <span class="mi">8</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">2</span><span class="n">x4</span><span class="p">]</span>
</code></pre>

<h3 id="torch-tensor-ndarray">torch.Tensor(ndarray)</h3>

<p>Creates a <code class="prettyprint">Tensor</code> from a NumPy <code class="prettyprint">ndarray</code>.
If the <code class="prettyprint">dtype</code> of the <code class="prettyprint">ndarray</code> is the same as the type of the <code class="prettyprint">Tensor</code> being created,
The underlying memory of both are shared, i.e. if the value of an element
in the <code class="prettyprint">ndarray</code> is changed, the corresponding value in the <code class="prettyprint">Tensor</code> changes,
and vice versa.</p>
<pre class="highlight python"><code><span class="c"># create a ndarray of dtype=int64</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="c"># create a LongTensor. Since they are the same type (int64), the memory is shared</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">1</span>
 <span class="mi">1</span>
 <span class="mi">0</span>
 <span class="mi">1</span>
 <span class="mi">1</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">10</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="mi">100</span>

<span class="c"># now create an IntTensor from the same ndarray.</span>
<span class="c"># The memory is not shared in this case as the dtype=int64 != IntTensor (int32)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">30000</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="mi">100</span>
<span class="c"># a did not change to the value 30000</span>
</code></pre>

<h2 id="numpy-conversion">NumPy Conversion</h2>

<h3 id="torch-from_numpy-ndarray">torch.from_numpy(ndarray)</h3>

<p>This is a convenience function similar to the constructor above.
Given a numpy <code class="prettyprint">ndarray</code>, it constructs a torch <code class="prettyprint">Tensor</code> of the same <code class="prettyprint">dtype</code>
as the numpy array.</p>

<p>For example, passing in an ndarray of dtype=float64 will create a torch.DoubleTensor</p>

<h3 id="tensor-numpy">Tensor.numpy()</h3>

<p>This is a member function on a tensor that converts a torch <code class="prettyprint">Tensor</code> to a
numpy <code class="prettyprint">ndarray</code>. The memory of the data of both objects is shared.
Hence, changing a value in the <code class="prettyprint">Tensor</code> will change the corresponding value in
the <code class="prettyprint">ndarray</code> and vice versa.</p>
<pre class="highlight python"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c"># creates a numpy array with dtype=float32 in this case</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">-</span><span class="mf">1.0453</span>  <span class="mf">1.4730</span> <span class="o">-</span><span class="mf">1.8990</span> <span class="o">-</span><span class="mf">0.7763</span>
 <span class="mf">1.8155</span>  <span class="mf">1.4004</span> <span class="o">-</span><span class="mf">1.5286</span>  <span class="mf">1.0420</span>
 <span class="mf">0.6551</span>  <span class="mf">1.0258</span>  <span class="mf">0.1152</span> <span class="o">-</span><span class="mf">0.3239</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="n">x4</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">1.04525673</span>  <span class="mf">1.4730444</span>  <span class="o">-</span><span class="mf">1.89899576</span> <span class="o">-</span><span class="mf">0.77626842</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">1.81549406</span>  <span class="mf">1.40035892</span> <span class="o">-</span><span class="mf">1.5286355</span>   <span class="mf">1.04199517</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.6551016</span>   <span class="mf">1.02575183</span>  <span class="mf">0.11520521</span> <span class="o">-</span><span class="mf">0.32391372</span><span class="p">]]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="p">[[</span> <span class="o">-</span><span class="mf">1.04525673e+00</span>   <span class="mf">1.47304440e+00</span>  <span class="o">-</span><span class="mf">1.89899576e+00</span>  <span class="o">-</span><span class="mf">7.76268423e-01</span><span class="p">]</span>
 <span class="p">[</span>  <span class="mf">1.81549406e+00</span>   <span class="mf">1.40035892e+00</span>  <span class="o">-</span><span class="mf">1.52863550e+00</span>   <span class="mf">1.04199517e+00</span><span class="p">]</span>
 <span class="p">[</span>  <span class="mf">6.55101597e-01</span>   <span class="mf">1.02575183e+00</span>   <span class="mf">1.00000000e+03</span>  <span class="o">-</span><span class="mf">3.23913723e-01</span><span class="p">]]</span>
<span class="c"># notice that b[2][2] has changed to the value 1000 too.</span>
</code></pre>

<h3 id="torch-is_tensor-obj">torch.is_tensor(obj)</h3>

<p>Returns True if the passed-in object is a <code class="prettyprint">Tensor</code> (of any type). Returns <code class="prettyprint">False</code> otherwise.</p>

<h3 id="torch-is_storage-obj">torch.is_storage(obj)</h3>

<p>Returns True if the passed-in object is a <code class="prettyprint">Storage</code> (of any type). Returns <code class="prettyprint">False</code> otherwise.</p>

<h3 id="torch-expand_as">torch.expand_as</h3>

<h3 id="torch-expand">torch.expand</h3>

<h3 id="torch-view">torch.view</h3>

<h3 id="torch-view_as">torch.view_as</h3>

<h3 id="torch-permute">torch.permute</h3>

<h3 id="torch-pin_memory">torch.pin_memory</h3>

<h3 id="copy">copy</h3>

<h3 id="split">split</h3>

<h3 id="chunk">chunk</h3>

<h3 id="tolist">tolist</h3>

<h3 id="repeat">repeat</h3>

<h3 id="unsqueeze">unsqueeze</h3>

<h3 id="unsqueeze_">unsqueeze_</h3>

<h3 id="add-iadd-sub-isub-mul-imul-matmul-div-rdiv-idiv-mod-neg">add, iadd, sub, isub, mul, imul, matmul, div, rdiv, idiv, mod, neg</h3>

<h2 id="gpu-semantics">GPU Semantics</h2>

<p>When you create a <code class="prettyprint">torch.cuda.*Tensor</code>, it is allocated on the current GPU.
However, you could allocate it on another GPU as well, using the <code class="prettyprint">with torch.cuda.device(id)</code> context.
All allocations within this context will be placed on the GPU <code class="prettyprint">id</code>.</p>

<p>Once <code class="prettyprint">Tensor</code>s are allocated, you can do operations on them from any GPU context, and the results
will be placed on the same device as where the source <code class="prettyprint">Tensor</code> is located.</p>

<p>For example if Tensor <code class="prettyprint">a</code> and <code class="prettyprint">b</code> are on GPU-2, but the GPU-1 is the current device.
If one does <code class="prettyprint">c = a + b</code>, then <code class="prettyprint">c</code> will be on GPU-2, regardless of what the current device is.</p>

<p>Cross-GPU operations are not allowed. The only Cross-GPU operation allowed is <code class="prettyprint">copy</code>.</p>

<p>If <code class="prettyprint">a</code> is on GPU-1 and <code class="prettyprint">b</code> is on GPU-2, then <code class="prettyprint">c = a + b</code> will result in an error.</p>

<p>See the example for more clarity on these semantics.</p>
<pre class="highlight python"><code><span class="c"># Tensors are allocated on GPU 1 by default</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c"># x.get_device() == 0</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="c"># y.get_device() == 0</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="c"># allocates a tensor on GPU 2</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c"># transfers a tensor from CPU to GPU-2</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="c"># a.get_device() == b.get_device() == 1</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="c"># z.get_device() == 1</span>

    <span class="c"># even within a context, you can give a GPU id to the .cuda call</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c"># c.get_device() == 2</span>

</code></pre>
