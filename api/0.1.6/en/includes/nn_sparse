<h2 id="sparse-layers">Sparse layers</h2>

<h3 id="embedding">Embedding</h3>

<p>A simple lookup table that stores embeddings of a fixed dictionary and size</p>
<pre class="highlight python"><code><span class="c"># an Embedding module containing 10 tensors of size 3</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c"># a batch of 2 samples of 4 indices each</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
<span class="c"># example with padding_idx</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_embeddings</td>
<td></td>
<td>size of the dictionary of embeddings</td>
</tr>
<tr>
<td>embedding_dim</td>
<td></td>
<td>the size of each embedding vector</td>
</tr>
<tr>
<td>padding_idx</td>
<td>None</td>
<td>If given, pads the output with zeros whenever it encounters the index.</td>
</tr>
<tr>
<td>max_norm</td>
<td>None</td>
<td>If given, will renormalize the embeddings to always have a norm lesser than this</td>
</tr>
<tr>
<td>norm_type</td>
<td></td>
<td>The p of the p-norm to compute for the max_norm option</td>
</tr>
<tr>
<td>scale_grad_by_freq</td>
<td></td>
<td>if given, this will scale gradients by the frequency of the words in the dictionary.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ *, * ]</td>
<td>Input is a 2D mini_batch LongTensor of m x n indices to extract from the Embedding dictionary</td>
</tr>
<tr>
<td>output</td>
<td>[ * , *, * ]</td>
<td>Output shape = m x n x embedding_dim</td>
</tr>
</tbody></table>
