<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>PyTorch API Reference</title>

    <style>
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight, .highlight .w {
  color: #f8f8f2;
  background-color: #272822;
}
.highlight .err {
  color: #151515;
  background-color: #ac4142;
}
.highlight .c, .highlight .cd, .highlight .cm, .highlight .c1, .highlight .cs {
  color: #505050;
}
.highlight .cp {
  color: #f4bf75;
}
.highlight .nt {
  color: #f4bf75;
}
.highlight .o, .highlight .ow {
  color: #d0d0d0;
}
.highlight .p, .highlight .pi {
  color: #d0d0d0;
}
.highlight .gi {
  color: #90a959;
}
.highlight .gd {
  color: #ac4142;
}
.highlight .gh {
  color: #6a9fb5;
  background-color: #151515;
  font-weight: bold;
}
.highlight .k, .highlight .kn, .highlight .kp, .highlight .kr, .highlight .kv {
  color: #aa759f;
}
.highlight .kc {
  color: #d28445;
}
.highlight .kt {
  color: #d28445;
}
.highlight .kd {
  color: #d28445;
}
.highlight .s, .highlight .sb, .highlight .sc, .highlight .sd, .highlight .s2, .highlight .sh, .highlight .sx, .highlight .s1 {
  color: #90a959;
}
.highlight .sr {
  color: #75b5aa;
}
.highlight .si {
  color: #8f5536;
}
.highlight .se {
  color: #8f5536;
}
.highlight .nn {
  color: #f4bf75;
}
.highlight .nc {
  color: #f4bf75;
}
.highlight .no {
  color: #f4bf75;
}
.highlight .na {
  color: #6a9fb5;
}
.highlight .m, .highlight .mf, .highlight .mh, .highlight .mi, .highlight .il, .highlight .mo, .highlight .mb, .highlight .mx {
  color: #90a959;
}
.highlight .ss {
  color: #90a959;
}
    </style>
    <link href="stylesheets/screen.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print.css" rel="stylesheet" media="print" />
      <script src="javascripts/all.js"></script>
  </head>

  <body class="index" data-languages="[]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar.png" />
      </span>
    </a>
    <div class="tocify-wrapper">
      <img src="images/logo.png" />
        <div class="lang-selector">
        </div>
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <div id="toc">
      </div>
        <ul class="toc-footer">
            <li><a href='https://github.com/tripit/slate'>Documentation Powered by Slate</a></li>
        </ul>
    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content"> 
        <h1 id="introduction">Introduction</h1>

<p>Welcome to the PyTorch API Reference.
Here you will find reference documentation for the built-in PyTorch packages.</p>

          <h1 id="torch">torch</h1>
<pre class="highlight python"><code><span class="c"># load torch with</span>
<span class="kn">import</span> <span class="nn">torch</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># load the CUDA features of torch with</span>
<span class="kn">import</span> <span class="nn">torch.cuda</span>
</code></pre>

<p><strong>torch</strong> is the main package where data structures for multi-dimensional
tensors and mathematical operations over these are defined.
Additionally, it provides many utilities for efficient serializing of
Tensors and arbitrary types, and other useful utilities.</p>

<p>It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability &gt;= 2.0.</p>

<h2 id="multi-core">Multi-core</h2>

<h3 id="torch-get_num_threads">torch.get_num_threads()</h3>

<p>Gets the number of OpenMP threads that will be used for parallelizing CPU operations</p>

<h3 id="torch-set_num_threads-n">torch.set_num_threads(n)</h3>

<p>Sets the number of OpenMP threads to use for parallelizing CPU operations</p>

<h2 id="serialization">Serialization</h2>

<h3 id="torch-save-object-file">torch.save(object, file)</h3>

<p>This function pickles a Python object to the <code class="prettyprint">file</code>. <code class="prettyprint">file</code> is either a filename or a file handle.</p>

<p><code class="prettyprint">object</code> can be a picklable python object, including <code class="prettyprint">torch</code> <code class="prettyprint">Tensor</code>s, autograd <code class="prettyprint">Variable</code>, nn <code class="prettyprint">Module</code>s etc.</p>

<p>When a group of <code class="prettyprint">torch</code> <code class="prettyprint">Tensor</code>s are saved together, and if any of them share the same storages, then this sharing is preserved during saving and loading back.</p>

<h3 id="torch-load-file">torch.load(file)</h3>

<p>This function unpickles objects that have been pickled with <code class="prettyprint">torch.save</code></p>

<h2 id="random-numbers">Random Numbers</h2>

<h3 id="torch-get_rng_state">torch.get_rng_state()</h3>

<p>Gets the current state of the torch Random Number Generator.</p>

<p>This can be passed in the future to <code class="prettyprint">torch.set_rng_state</code> to restore the current RNG state.</p>

<h3 id="torch-set_rng_state-state">torch.set_rng_state(state)</h3>

<p>Sets the current state of the torch Random Number Generator to the given <code class="prettyprint">state</code>. </p>

<h3 id="torch-manual_seed-number">torch.manual_seed(number)</h3>

<p>Sets the initial seed of the random number generator to a given number.</p>

<h3 id="torch-initial_seed">torch.initial_seed()</h3>

<p>Returns the number that is the initial seed to the Random Number Generator</p>

<h2 id="cuda">CUDA</h2>

<h3 id="torch-cuda-is_available">torch.cuda.is_available()</h3>

<p>Returns <code class="prettyprint">True</code> if CUDA is available and usable. Returns <code class="prettyprint">False</code> otherwise.</p>

<h3 id="torch-cuda-device_count">torch.cuda.device_count()</h3>

<p>Returns the number of CUDA devices on the system.</p>

<h3 id="torch-cuda-current_device">torch.cuda.current_device()</h3>

<p>Returns the device index of the current default CUDA device.</p>

<h3 id="torch-cuda-synchronize">torch.cuda.synchronize()</h3>

<p>This function issues a <code class="prettyprint">cudaDeviceSynchronize</code> on the current device, and hence waits for all in-flight CUDA computation to finish.</p>

<h3 id="torch-cuda-current_stream">torch.cuda.current_stream()</h3>

<p>Returns the handle to the current stream of the CUDA context.</p>

          <h1 id="tensors">Tensors</h1>

<p>A <code class="prettyprint">Tensor</code> is a potentially multi-dimensional matrix.
The number of dimensions is unlimited.</p>

<p>The <code class="prettyprint">Tensor</code> set of classes are probably the most important class in
<code class="prettyprint">torch</code>. Almost every package depends on these classes. They are <em><strong>the</strong></em>
class for handling numeric data. As with pretty much anything in
[torch], tensors are serializable with <code class="prettyprint">torch.save</code> and <code class="prettyprint">torch.load</code></p>

<p>There are 7 Tensor classes in torch:</p>

<ul>
<li><code class="prettyprint">torch.FloatTensor</code>   :   Signed 32-bit floating point tensor</li>
<li><code class="prettyprint">torch.DoubleTensor</code>  :   Signed 64-bit floating point tensor</li>
<li><code class="prettyprint">torch.ByteTensor</code>    :   Signed  8-bit integer tensor</li>
<li><code class="prettyprint">torch.CharTensor</code>    : Unsigned  8-bit integer tensor</li>
<li><code class="prettyprint">torch.ShortTensor</code>   :   Signed 16-bit integer tensor</li>
<li><code class="prettyprint">torch.IntTensor</code>     :   Signed 32-bit integer tensor</li>
<li><code class="prettyprint">torch.LongTensor</code>    :   Signed 64-bit integer tensor</li>
</ul>

<p>The data in these tensors lives on the system memory connected to your CPU.</p>

<p>Most numeric operations are implemented <em>only</em> for <code class="prettyprint">FloatTensor</code> and <code class="prettyprint">DoubleTensor</code>.
Other Tensor types are useful if you want to save memory space or specifically
do integer operations.</p>

<p>The number of dimensions of a <code class="prettyprint">Tensor</code> can be queried by
<code class="prettyprint">ndimension()</code> or <code class="prettyprint">dim()</code>. Size of the <code class="prettyprint">i-th</code> dimension is
returned by <code class="prettyprint">size(i)</code>. A tuple containing the size of all the dimensions
can be returned by <code class="prettyprint">size()</code>.</p>
<pre class="highlight python"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="c"># allocate a matrix of shape 3x4</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c"># convert this into a LongTensor</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="nb">long</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="c"># print the size of the tensor</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<span class="c"># print the number of dimensions</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dim</span><span class="p">())</span>
</code></pre>

<p>These tensors can be converted to numpy arrays very efficiently
with zero memory copies.
For this, the two provided functions are <code class="prettyprint">.numpy()</code> and <code class="prettyprint">torch.from_numpy()</code></p>
<pre class="highlight python"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c"># convert to numpy</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
</code></pre>

<p>When using GPUs, each of the classes above has an equivalent
class such as: <code class="prettyprint">torch.cuda.FloatTensor</code>, <code class="prettyprint">torch.cuda.LongTensor</code>, etc.
When one allocates a CUDA tensor, the data in these tensors lives in the
GPU memory.</p>

<p>One can seamlessly transfer a tensor from the CPU to the GPU, as well as
between different GPUs on your machine.</p>

<p>Apart from the above 7 tensor types, there is one additional tensor type on the GPU</p>

<ul>
<li><code class="prettyprint">torch.cuda.HalfTensor</code> : Signed 16-bit floating point tensor</li>
</ul>
<pre class="highlight python"><code><span class="kn">import</span> <span class="nn">torch.cuda</span>

<span class="c"># allocate a matrix of shape 3x4</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c"># transfer this to the CPU</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="c"># transfer this back to the GPU-1</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c"># transfer this to GPU-2</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre>

<h2 id="internal-data-representation">Internal data representation</h2>

<p>The actual data of a <code class="prettyprint">Tensor</code> is contained into a
<code class="prettyprint">Storage</code>. It can be accessed using
<code class="prettyprint">storage()</code>. While the memory of a
<code class="prettyprint">Tensor</code> has to be contained in this unique <code class="prettyprint">Storage</code>, it might
not be contiguous: the first position used in the <code class="prettyprint">Storage</code> is given
by <code class="prettyprint">storage_offset()</code> (starting at <code class="prettyprint">0</code>).
And the <em>jump</em> needed to go from one element to another
element in the <code class="prettyprint">i-th</code> dimension is given by
<code class="prettyprint">stride(i-1)</code>. See the code example for an illustration.</p>
<pre class="highlight python"><code><span class="c"># given a 3d tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>

<span class="c"># accessing the element `(3,4,5)` can be done by</span>
<span class="n">x</span><span class="p">[</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span class="c"># or equivalently (but slowly!)</span>
<span class="n">x</span><span class="o">.</span><span class="n">storage</span><span class="p">()[</span><span class="n">x</span><span class="o">.</span><span class="n">storageOffset</span><span class="p">()</span>
            <span class="o">+</span> <span class="p">(</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="p">(</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="o">+</span> <span class="p">(</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
</code></pre>

<p>One could say that a <code class="prettyprint">Tensor</code> is a particular way of <em>viewing</em> a
<code class="prettyprint">Storage</code>: a <code class="prettyprint">Storage</code> only represents a chunk of memory, while the
<code class="prettyprint">Tensor</code> interprets this chunk of memory as having dimensions:</p>
<pre class="highlight python"><code><span class="c"># a tensor interprets a chunk of memory as having dimensions</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">s</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span> <span class="c"># fill up the Storage</span>
<span class="o">&gt;&gt;&gt;</span>   <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

<span class="c"># s is interpreted by x as a 2D matrix</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="mi">1</span>   <span class="mi">2</span>   <span class="mi">3</span>   <span class="mi">4</span>   <span class="mi">5</span>
  <span class="mi">6</span>   <span class="mi">7</span>   <span class="mi">8</span>   <span class="mi">9</span>  <span class="mi">10</span>
 <span class="mi">11</span>  <span class="mi">12</span>  <span class="mi">13</span>  <span class="mi">14</span>  <span class="mi">15</span>
 <span class="mi">16</span>  <span class="mi">17</span>  <span class="mi">18</span>  <span class="mi">19</span>  <span class="mi">20</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">4</span><span class="n">x5</span><span class="p">]</span>
</code></pre>

<p>Note also that in Torch7 <strong><em>elements in the same row</em></strong> [elements along the <strong>last</strong> dimension]
are contiguous in memory for a matrix [tensor]:</p>

<p>This is exactly like in <code class="prettyprint">C</code> and <code class="prettyprint">numpy</code> (and not <code class="prettyprint">Fortran</code>).</p>

<h2 id="default-tensor-type">Default Tensor type</h2>

<p>For convenience, <em>an alias</em> <code class="prettyprint">torch.Tensor</code> is provided, which allows the user to write
type-independent scripts, which can then ran after choosing the desired Tensor type with
a call like</p>

<p><code class="prettyprint">torch.set_default_tensor_type(&#39;torch.DoubleTensor&#39;)</code></p>

<p>By default, the alias points to <code class="prettyprint">torch.FloatTensor</code>.</p>

<h2 id="efficient-memory-management">Efficient memory management</h2>

<p><em>All</em> tensor operations post-fixed with an underscore (for example <code class="prettyprint">.fill_</code>)
do <em>not</em> make any memory copy. All these methods transform the existing tensor.
Tensor methods such as <code class="prettyprint">narrow</code> and <code class="prettyprint">select</code> return a new tensor referencing <em>the same storage</em>.
This magical behavior is internally obtained by good usage of the <code class="prettyprint">stride()</code> and
<code class="prettyprint">storage_offset()</code>. See the code example illustrating this.</p>
<pre class="highlight python"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">5</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># narrow() returns a Tensor referencing the same Storage as x</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="mi">0</span>
 <span class="mi">1</span>
 <span class="mi">1</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">5</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c"># same thing can be achieved with slice indexing</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
 <span class="mi">0</span>
 <span class="mi">2</span>
 <span class="mi">2</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">5</span><span class="p">]</span>
</code></pre>

<p>If you really need to copy a <code class="prettyprint">Tensor</code>, you can use the <code class="prettyprint">copy_()</code> method:</p>
<pre class="highlight python"><code><span class="c"># making a copy of a tensor</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</code></pre>

<p>Or the convenience method <code class="prettyprint">clone()</code></p>

<p>We now describe all the methods for <code class="prettyprint">Tensor</code>. If you want to specify the Tensor type,
just replace <code class="prettyprint">Tensor</code> by the name of the Tensor variant (like <code class="prettyprint">CharTensor</code>).</p>

<h2 id="constructors">Constructors</h2>

<p>Tensor constructors, create new Tensor object, optionally, allocating
new memory. By default the elements of a newly allocated memory are
not initialized, therefore, might contain arbitrary numbers. Here are
several ways to construct a new <code class="prettyprint">Tensor</code>.</p>

<h3 id="torch-tensor">torch.Tensor()</h3>

<p>Returns an empty tensor.</p>

<h3 id="torch-tensor-tensor">torch.Tensor(tensor)</h3>

<p>Returns a new tensor which reference the same <code class="prettyprint">Storage</code> than the given <code class="prettyprint">tensor</code>.
The <code class="prettyprint">size</code>, <code class="prettyprint">stride</code>, and <code class="prettyprint">storage_offset</code> are the same than the given tensor.</p>

<p>The new <code class="prettyprint">Tensor</code> is now going to &ldquo;view&rdquo; the same <code class="prettyprint">storage</code>
as the given <code class="prettyprint">tensor</code>. As a result, any modification in the elements
of the <code class="prettyprint">Tensor</code> will have a impact on the elements of the given
<code class="prettyprint">tensor</code>, and vice-versa. No memory copy!</p>
<pre class="highlight python"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">3.14</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
 <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>
 <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">2</span><span class="n">x5</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span>
 <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>
 <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>  <span class="mf">3.1400</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">2</span><span class="n">x5</span><span class="p">]</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="c"># elements of x are the same as y!</span>
<span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">2</span><span class="n">x5</span><span class="p">]</span>
</code></pre>

<h3 id="torch-tensor-sz1-sz2-sz3-sz4-sz5">torch.Tensor(sz1 [,sz2 [,sz3 [,sz4 [,sz5 &hellip;]]]]])</h3>

<p>Create a tensor of the given sizes.
The tensor size will be <code class="prettyprint">sz1 x sz2 x sx3 x sz4 x sz5 x ...</code>.</p>

<h3 id="torch-tensor-sizes">torch.Tensor(sizes)</h3>

<p>Create a tensor of any number of dimensions. <code class="prettyprint">sizes</code> gives the size in each dimension of
the tensor and is of type <code class="prettyprint">torch.Size</code>.</p>
<pre class="highlight python"><code><span class="n">Example</span><span class="p">,</span> <span class="n">create</span> <span class="n">a</span> <span class="mi">4</span><span class="n">D</span> <span class="mi">4</span><span class="n">x4x3x2</span> <span class="n">tensor</span><span class="p">:</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]))</span>
</code></pre>

<h3 id="torch-tensor-storage">torch.Tensor(storage)</h3>

<p>Returns a tensor which uses the existing <code class="prettyprint">Storage</code> starting at a storage offset of 0.</p>

<h3 id="torch-tensor-sequence">torch.Tensor(sequence)</h3>

<p>One can create a tensor from a python sequence.</p>

<p>For example, you can create a <code class="prettyprint">Tensor</code> from a <code class="prettyprint">list</code> or a <code class="prettyprint">tuple</code></p>
<pre class="highlight python"><code><span class="c"># create a 2d tensor from a list of lists</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">]])</span>
 <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>  <span class="mi">4</span>
 <span class="mi">5</span>  <span class="mi">6</span>  <span class="mi">7</span>  <span class="mi">8</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">dimension</span> <span class="mi">2</span><span class="n">x4</span><span class="p">]</span>
</code></pre>

<h3 id="torch-tensor-ndarray">torch.Tensor(ndarray)</h3>

<p>Creates a <code class="prettyprint">Tensor</code> from a NumPy <code class="prettyprint">ndarray</code>.
If the <code class="prettyprint">dtype</code> of the <code class="prettyprint">ndarray</code> is the same as the type of the <code class="prettyprint">Tensor</code> being created,
The underlying memory of both are shared, i.e. if the value of an element
in the <code class="prettyprint">ndarray</code> is changed, the corresponding value in the <code class="prettyprint">Tensor</code> changes,
and vice versa.</p>
<pre class="highlight python"><code><span class="c"># create a ndarray of dtype=int64</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
<span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="c"># create a LongTensor. Since they are the same type (int64), the memory is shared</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">1</span>
 <span class="mi">1</span>
 <span class="mi">0</span>
 <span class="mi">1</span>
 <span class="mi">1</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
 <span class="mi">0</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">10</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="mi">100</span>

<span class="c"># now create an IntTensor from the same ndarray.</span>
<span class="c"># The memory is not shared in this case as the dtype=int64 != IntTensor (int32)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">30000</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="mi">100</span>
<span class="c"># a did not change to the value 30000</span>
</code></pre>

<h2 id="numpy-conversion">NumPy Conversion</h2>

<h3 id="torch-from_numpy-ndarray">torch.from_numpy(ndarray)</h3>

<p>This is a convenience function similar to the constructor above.
Given a numpy <code class="prettyprint">ndarray</code>, it constructs a torch <code class="prettyprint">Tensor</code> of the same <code class="prettyprint">dtype</code>
as the numpy array.</p>

<p>For example, passing in an ndarray of dtype=float64 will create a torch.DoubleTensor</p>

<h3 id="tensor-numpy">Tensor.numpy()</h3>

<p>This is a member function on a tensor that converts a torch <code class="prettyprint">Tensor</code> to a
numpy <code class="prettyprint">ndarray</code>. The memory of the data of both objects is shared.
Hence, changing a value in the <code class="prettyprint">Tensor</code> will change the corresponding value in
the <code class="prettyprint">ndarray</code> and vice versa.</p>
<pre class="highlight python"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c"># creates a numpy array with dtype=float32 in this case</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">-</span><span class="mf">1.0453</span>  <span class="mf">1.4730</span> <span class="o">-</span><span class="mf">1.8990</span> <span class="o">-</span><span class="mf">0.7763</span>
 <span class="mf">1.8155</span>  <span class="mf">1.4004</span> <span class="o">-</span><span class="mf">1.5286</span>  <span class="mf">1.0420</span>
 <span class="mf">0.6551</span>  <span class="mf">1.0258</span>  <span class="mf">0.1152</span> <span class="o">-</span><span class="mf">0.3239</span>
<span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">3</span><span class="n">x4</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">1.04525673</span>  <span class="mf">1.4730444</span>  <span class="o">-</span><span class="mf">1.89899576</span> <span class="o">-</span><span class="mf">0.77626842</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">1.81549406</span>  <span class="mf">1.40035892</span> <span class="o">-</span><span class="mf">1.5286355</span>   <span class="mf">1.04199517</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.6551016</span>   <span class="mf">1.02575183</span>  <span class="mf">0.11520521</span> <span class="o">-</span><span class="mf">0.32391372</span><span class="p">]]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="p">[[</span> <span class="o">-</span><span class="mf">1.04525673e+00</span>   <span class="mf">1.47304440e+00</span>  <span class="o">-</span><span class="mf">1.89899576e+00</span>  <span class="o">-</span><span class="mf">7.76268423e-01</span><span class="p">]</span>
 <span class="p">[</span>  <span class="mf">1.81549406e+00</span>   <span class="mf">1.40035892e+00</span>  <span class="o">-</span><span class="mf">1.52863550e+00</span>   <span class="mf">1.04199517e+00</span><span class="p">]</span>
 <span class="p">[</span>  <span class="mf">6.55101597e-01</span>   <span class="mf">1.02575183e+00</span>   <span class="mf">1.00000000e+03</span>  <span class="o">-</span><span class="mf">3.23913723e-01</span><span class="p">]]</span>
<span class="c"># notice that b[2][2] has changed to the value 1000 too.</span>
</code></pre>

<h3 id="torch-is_tensor-obj">torch.is_tensor(obj)</h3>

<p>Returns True if the passed-in object is a <code class="prettyprint">Tensor</code> (of any type). Returns <code class="prettyprint">False</code> otherwise.</p>

<h3 id="torch-is_storage-obj">torch.is_storage(obj)</h3>

<p>Returns True if the passed-in object is a <code class="prettyprint">Storage</code> (of any type). Returns <code class="prettyprint">False</code> otherwise.</p>

<h3 id="torch-expand_as">torch.expand_as</h3>

<h3 id="torch-expand">torch.expand</h3>

<h3 id="torch-view">torch.view</h3>

<h3 id="torch-view_as">torch.view_as</h3>

<h3 id="torch-permute">torch.permute</h3>

<h3 id="torch-pin_memory">torch.pin_memory</h3>

<h3 id="copy">copy</h3>

<h3 id="split">split</h3>

<h3 id="chunk">chunk</h3>

<h3 id="tolist">tolist</h3>

<h3 id="repeat">repeat</h3>

<h3 id="unsqueeze">unsqueeze</h3>

<h3 id="unsqueeze_">unsqueeze_</h3>

<h3 id="add-iadd-sub-isub-mul-imul-matmul-div-rdiv-idiv-mod-neg">add, iadd, sub, isub, mul, imul, matmul, div, rdiv, idiv, mod, neg</h3>

<h2 id="gpu-semantics">GPU Semantics</h2>

<p>When you create a <code class="prettyprint">torch.cuda.*Tensor</code>, it is allocated on the current GPU.
However, you could allocate it on another GPU as well, using the <code class="prettyprint">with torch.cuda.device(id)</code> context.
All allocations within this context will be placed on the GPU <code class="prettyprint">id</code>.</p>

<p>Once <code class="prettyprint">Tensor</code>s are allocated, you can do operations on them from any GPU context, and the results
will be placed on the same device as where the source <code class="prettyprint">Tensor</code> is located.</p>

<p>For example if Tensor <code class="prettyprint">a</code> and <code class="prettyprint">b</code> are on GPU-2, but the GPU-1 is the current device.
If one does <code class="prettyprint">c = a + b</code>, then <code class="prettyprint">c</code> will be on GPU-2, regardless of what the current device is.</p>

<p>Cross-GPU operations are not allowed. The only Cross-GPU operation allowed is <code class="prettyprint">copy</code>.</p>

<p>If <code class="prettyprint">a</code> is on GPU-1 and <code class="prettyprint">b</code> is on GPU-2, then <code class="prettyprint">c = a + b</code> will result in an error.</p>

<p>See the example for more clarity on these semantics.</p>
<pre class="highlight python"><code><span class="c"># Tensors are allocated on GPU 1 by default</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c"># x.get_device() == 0</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="c"># y.get_device() == 0</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="c"># allocates a tensor on GPU 2</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c"># transfers a tensor from CPU to GPU-2</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="c"># a.get_device() == b.get_device() == 1</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="c"># z.get_device() == 1</span>

    <span class="c"># even within a context, you can give a GPU id to the .cuda call</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="c"># c.get_device() == 2</span>

</code></pre>

          <h1 id="torch-nn">torch.nn</h1>

<p>Neural Networks in PyTorch</p>

          <h2 id="containers">Containers</h2>

<h3 id="container">Container</h3>

<p>This is the base container class for all neural networks you would define.</p>
<pre class="highlight python"><code><span class="c"># Example of using Container</span>
 <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Container</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
            <span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
            <span class="n">relu</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
         <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">output</span>
 <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># one can add modules to the container after construction</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s">'pool1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</code></pre>
<pre class="highlight python"><code></code></pre>
<pre class="highlight python"><code><span class="c"># .parameters()</span>
</code></pre>
<pre class="highlight python"><code><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">'</span><span class="nc">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="s">'&gt; (20L,)</span><span class="err">
</span><span class="s">&lt;class '</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="s">'&gt; (20L, 1L, 5L, 5L)</span><span class="err">
</span></code></pre>
<pre class="highlight python"><code></code></pre>
<pre class="highlight python"><code><span class="c"># .state_dict()</span>
</code></pre>
<pre class="highlight python"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">pdict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">sdict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="p">[</span><span class="s">'conv1.bias'</span><span class="p">,</span> <span class="s">'conv1.weight'</span><span class="p">]</span>
</code></pre>
<pre class="highlight python"><code></code></pre>

<p>You will subclass your container from this class.
In the constructor you define the modules that you would want to use,
and in the &ldquo;forward&rdquo; function you use the constructed modules in
your operations.</p>

<p>To make it easier to understand, given is a small example.</p>

<p>One can also add new modules to a container after construction.
You can do this with the add_module function 
or by assigning them as Container attributes.</p>

<h4 id="one-can-also-set-modules-as-attributes-of-the-container">one can also set modules as attributes of the container</h4>

<p>model.conv1 = nn.Conv2d(12, 24, 3)
The container has some important additional methods: </p>

<p><strong><code class="prettyprint">[generator] parameters()</code></strong></p>

<p>returns a generator over all learnable parameters in the container instance. 
This can typically be passed to the optimizer API</p>

<p><strong><code class="prettyprint">[dict] state_dict()</code></strong></p>

<p>returns a dictionary of learnable parameters of the Container.
For example: [&lsquo;conv1.weight&rsquo; : Parameter(torch.FloatTensor(20x1x5x5)),
              &#39;conv1.bias&rsquo;   : Parameter(torch.FloatTensor(20)),
             ]</p>

<p><strong><code class="prettyprint">load_state_dict(dict)</code></strong></p>

<p>Given a parameter dict, sets the parameters of self to be the given dict.
It loads loads the parameters recursively.
Excessive or non-matching parameter names are ignored.
For example, the input dict has an entry &#39;conv44.weight&rsquo;, but 
if the container does not have a module named &#39;conv44&rsquo;, then this entry is ignored.</p>

<p><strong><code class="prettyprint">children()</code></strong></p>

<p>Returns a generator over all the children modules of self</p>

<p><strong><code class="prettyprint">train()</code></strong></p>

<p>Sets the Container (and all it&rsquo;s child modules) to training mode (for modules such as batchnorm, dropout etc.)</p>

<p><strong><code class="prettyprint">eval()</code></strong></p>

<p>Sets the Container (and all it&rsquo;s child modules) to evaluate mode (for modules such as batchnorm, dropout etc.)</p>

<p><strong><code class="prettyprint">apply(closure)</code></strong></p>

<p>Applies the given closure to each parameter of the container. </p>

<p><strong>_<em>Note: Apart from these, the container will define the base functions that it has derived from nn.Module _</em></strong></p>

<h3 id="sequential">Sequential</h3>

<p>A sequential Container. It is derived from the base nn.Container class</p>
<pre class="highlight python"><code><span class="c"># Example of using Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
</code></pre>
<pre class="highlight python"><code></code></pre>

<p>Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>

<p>To make it easier to understand, given is a small example.</p>

<h4 id="example-of-using-sequential-with-ordereddict">Example of using Sequential with OrderedDict</h4>

<p>model = nn.Sequential(OrderedDict([
          (&#39;conv1&rsquo;, nn.Conv2d(1,20,5)),
          (&#39;relu1&rsquo;, nn.ReLU()),
          (&#39;conv2&rsquo;, nn.Conv2d(20,64,5)),
          (&#39;relu2&rsquo;, nn.ReLU())
        ]))</p>

          <h2 id="convolution-layers">Convolution Layers</h2>

<h3 id="conv1d">Conv1d</h3>

<p>Applies a 1D convolution over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">iC</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">oC</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">oc_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span><span class="p">[</span><span class="n">oc_i</span><span class="p">]</span>
            <span class="o">+</span> <span class="n">sum_iC</span> <span class="n">sum_</span><span class="p">{</span><span class="n">ow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">oW</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kw</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">kW</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span>
                <span class="n">weight</span><span class="p">[</span><span class="n">oc_i</span><span class="p">][</span><span class="n">ic_i</span><span class="p">][</span><span class="n">kw</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">ic_i</span><span class="p">][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">ow</span> <span class="o">+</span> <span class="n">kw</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.</p>

<p>Note that depending of the size of your kernel, several (of the last)
columns of the input might be lost. It is up to the user
to add proper padding.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel.</td>
</tr>
<tr>
<td>stride</td>
<td></td>
<td>the stride of the convolving kernel.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * ]</td>
<td>Input is minibatch x in_channels x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * ]</td>
<td>Output shape is precisely minibatch x out_channels x floor((iW  + 2*padW - kW) / dW + 1)</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>the learnable weights of the module of shape (out_channels x in_channels x kW)</td>
</tr>
<tr>
<td>bias</td>
<td>the learnable bias of the module of shape (out_channels)</td>
</tr>
</tbody></table>

<h3 id="conv2d">Conv2d</h3>

<p>Applies a 2D convolution over an input image composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">iC</span> <span class="n">x</span> <span class="n">H</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">oC</span> <span class="n">x</span> <span class="n">oH</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">oc_i</span><span class="p">][</span><span class="n">h_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span><span class="p">[</span><span class="n">oc_i</span><span class="p">]</span>
            <span class="o">+</span> <span class="n">sum_iC</span> <span class="n">sum_</span><span class="p">{</span><span class="n">oh</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">oH</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">ow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">oW</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kh</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">kH</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kw</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">kW</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span>
                <span class="n">weight</span><span class="p">[</span><span class="n">oc_i</span><span class="p">][</span><span class="n">ic_i</span><span class="p">][</span><span class="n">kh</span><span class="p">][</span><span class="n">kw</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">ic_i</span><span class="p">][</span><span class="n">stride_h</span> <span class="o">*</span> <span class="n">oh</span> <span class="o">+</span> <span class="n">kh</span><span class="p">)][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">ow</span> <span class="o">+</span> <span class="n">kw</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With square kernels and equal stride</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># non-square kernels and unequal stride and with padding</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="c"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.</p>

<p>Note that depending of the size of your kernel, several (of the last)
columns or rows of the input image might be lost. It is up to the user
to add proper padding in images.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>1</td>
<td>the stride of the convolving kernel. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit zero padding on the input. Can be a single number s or a tuple.</td>
</tr>
<tr>
<td>dilation</td>
<td>None</td>
<td>If given, will do dilated (or atrous) convolutions. Can be a single number s or a tuple.</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If set to False, the layer will not learn an additive bias.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * , * ]</td>
<td>Input is minibatch x in_channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * , * ]</td>
<td>Output shape is precisely minibatch x out_channels x floor((iH  + 2*padH - kH) / dH + 1) x floor((iW  + 2*padW - kW) / dW + 1)</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>the learnable weights of the module of shape (out_channels x in_channels x kH x kW)</td>
</tr>
<tr>
<td>bias</td>
<td>the learnable bias of the module of shape (out_channels)</td>
</tr>
</tbody></table>

<h3 id="convtranspose2d">ConvTranspose2d</h3>

<p>Applies a 2D deconvolution operator over an input image composed of several input</p>
<pre class="highlight python"><code><span class="c"># With square kernels and equal stride</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># non-square kernels and unequal stride and with padding</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="c"># exact output size can be also specified as an argument</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre>

<p>planes.
The deconvolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.
This module can be seen as the exact reverse of the Conv2d module.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>1</td>
<td>the stride of the convolving kernel. Can be a single number or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit zero padding on the input. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>output_padding</td>
<td>0</td>
<td>A zero-padding of 0 &lt;= padding &lt; stride that should be added to the output. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If set to False, the layer will not learn an additive bias.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * , * ]</td>
<td>Input is minibatch x in_channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * , * ]</td>
<td>Output shape is minibatch x out_channels x (iH - 1) * sH - 2*padH + kH + output_paddingH x (iW - 1) * sW - 2*padW + kW, or as specified in a second argument to the call.</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>the learnable weights of the module of shape (in_channels x out_channels x kH x kW)</td>
</tr>
<tr>
<td>bias</td>
<td>the learnable bias of the module of shape (out_channels)</td>
</tr>
</tbody></table>

<h3 id="conv3d">Conv3d</h3>

<p>Applies a 3D convolution over an input image composed of several input</p>
<pre class="highlight python"><code><span class="c"># With square kernels and equal stride</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># non-square kernels and unequal stride and with padding</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.</p>

<p>Note that depending of the size of your kernel, several (of the last)
columns or rows of the input image might be lost. It is up to the user
to add proper padding in images.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel. Can be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>1</td>
<td>the stride of the convolving kernel. Can be a single number s or a tuple (kt x sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit zero padding on the input. Can be a single number s or a tuple.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * , * , * ]</td>
<td>Input is minibatch x in_channels x iT x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * , * , * ]</td>
<td>Output shape is precisely minibatch x out_channels x floor((iT  + 2*padT - kT) / dT + 1) x floor((iH  + 2*padH - kH) / dH + 1) x floor((iW  + 2*padW - kW) / dW + 1)</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>the learnable weights of the module of shape (out_channels x in_channels x kT x kH x kW)</td>
</tr>
<tr>
<td>bias</td>
<td>the learnable bias of the module of shape (out_channels)</td>
</tr>
</tbody></table>

<h3 id="convtranspose3d">ConvTranspose3d</h3>

<p>Applies a 3D deconvolution operator over an input image composed of several input</p>
<pre class="highlight python"><code><span class="c"># With square kernels and equal stride</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># non-square kernels and unequal stride and with padding</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.
The deconvolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.
This module can be seen as the exact reverse of the Conv3d module.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel. Can be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>1</td>
<td>the stride of the convolving kernel. Can be a single number or a tuple (st x sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit zero padding on the input. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>output_padding</td>
<td>0</td>
<td>A zero-padding of 0 &lt;= padding &lt; stride that should be added to the output. Can be a single number or a tuple.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * , * , * ]</td>
<td>Input is minibatch x in_channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * , * , * ]</td>
<td>Output shape is precisely minibatch x out_channels x (iT - 1) * sT - 2*padT + kT + output_paddingT x (iH - 1) * sH - 2*padH + kH + output_paddingH x (iW - 1) * sW - 2*padW + kW</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>the learnable weights of the module of shape (in_channels x out_channels x kT x kH x kW)</td>
</tr>
<tr>
<td>bias</td>
<td>the learnable bias of the module of shape (out_channels)</td>
</tr>
</tbody></table>

          <h2 id="pooling-layers">Pooling Layers</h2>

<h3 id="maxpool1d">MaxPool1d</h3>

<p>Applies a 1D max pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_</span><span class="p">{</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">}</span> <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">w_i</span> <span class="o">+</span> <span class="n">k</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># pool of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a max over</td>
</tr>
<tr>
<td>stride</td>
<td></td>
<td>the stride of the window</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding to be added.</td>
</tr>
<tr>
<td>dilation</td>
<td>kernel_size</td>
<td>a parameter that controls the stride of elements in the window.</td>
</tr>
<tr>
<td>return_indices</td>
<td>False</td>
<td>if True, will return the indices along with the outputs. Useful when Unpooling later.</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , * ]</td>
<td>Input is minibatch x channels x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , * ]</td>
<td>Output shape = minibatch x channels x floor((iW  + 2*padW - kernel_size) / stride + 1)</td>
</tr>
</tbody></table>

<h3 id="maxpool2d">MaxPool2d</h3>

<p>Applies a 2D max pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">H</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">oH</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">h_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_</span><span class="p">{{</span><span class="n">kh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">KH</span><span class="p">},</span> <span class="p">{</span><span class="n">kw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kW</span><span class="p">}}</span> <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">stride_h</span> <span class="o">*</span> <span class="n">h_i</span> <span class="o">+</span> <span class="n">kH</span><span class="p">)][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">w_i</span> <span class="o">+</span> <span class="n">kW</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding to be added. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>dilation</td>
<td>1</td>
<td>a parameter that controls the stride of elements in the window. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>return_indices</td>
<td>False</td>
<td>if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d .</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape = minibatch x channels x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h3 id="maxpool3d">MaxPool3d</h3>

<p>Applies a 3D max pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a max over. Can be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (st x sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding to be added. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>dilation</td>
<td>1</td>
<td>a parameter that controls the stride of elements in the window. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>return_indices</td>
<td>False</td>
<td>if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d .</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, *, * ]</td>
<td>Input is minibatch x channels x iT x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, *, * ]</td>
<td>Output shape = minibatch x channels x floor((iT  + 2*padT - kT) / sT + 1) x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h3 id="maxunpool2d">MaxUnpool2d</h3>

<p>Computes the inverse operation of MaxPool2d</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">unpooled_output</span> <span class="o">=</span> <span class="n">mu</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="c"># exact output size can be also specified as an argument</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
<span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">upsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">h</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre>

<p>MaxPool2d is not invertible, as the locations of the max locations are lost.
MaxUnpool2d takes in as input the output of MaxPool2d and the indices of the Max locations
and computes the inverse.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the max window. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding that was added to the input. Can be a single number or a tuple.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape is minibatch x channels x padH x (iH - 1) * sH + kH x padW x (iW - 1) * sW + kW, or as specified to the call.</td>
</tr>
</tbody></table>

<h3 id="maxunpool3d">MaxUnpool3d</h3>

<p>Computes the inverse operation of MaxPool3d</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">unpooled_output</span> <span class="o">=</span> <span class="n">m2</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</code></pre>

<p>MaxPool3d is not invertible, as the locations of the max locations are lost.
MaxUnpool3d takes in as input the output of MaxPool3d and the indices of the Max locations
and computes the inverse.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the max window. Can be a single number k (for a square kernel of k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (st x sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding that was added to the input. Can be a single number or a tuple.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, *, * ]</td>
<td>Input is minibatch x channels x iT x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, *, * ]</td>
<td>Output shape = minibatch x channels x padT x (iT - 1) * sT + kT x padH x (iH - 1) * sH + kH x padW x (iW - 1) * sW + kW</td>
</tr>
</tbody></table>

<h3 id="avgpool2d">AvgPool2d</h3>

<p>Applies a 2D average pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">H</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">oH</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">h_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span> <span class="o">*</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">KH</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kW</span><span class="p">}</span>  <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">stride_h</span> <span class="o">*</span> <span class="n">h_i</span> <span class="o">+</span> <span class="n">kh</span><span class="p">)][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">w_i</span> <span class="o">+</span> <span class="n">kw</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding to be added. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape = minibatch x channels x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h3 id="avgpool3d">AvgPool3d</h3>

<p>Applies a 3D average pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a average over. Can be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (st x sh x sw).</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, *, * ]</td>
<td>Input is minibatch x channels x iT x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, *, * ]</td>
<td>Output shape = minibatch x channels x floor((iT  + 2*padT - kT) / sT + 1) x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h3 id="fractionalmaxpool2d">FractionalMaxPool2d</h3>

<p>Applies a 2D fractional max pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, and target output size 13x12</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="c"># pool of square window and target output size being half of input image size</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.</p>

<p>Fractiona MaxPooling is described in detail in the paper <a href="http://arxiv.org/abs/1412.6071">&ldquo;Fractional Max-Pooling&rdquo; by Ben Graham</a>
The max-pooling operation is applied in kHxkW regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>output_size</td>
<td></td>
<td>the target output size of the image of the form oH x oW. Can be a tuple (oH, oW) or a single number oH for a square image oH x oH</td>
</tr>
<tr>
<td>output_ratio</td>
<td></td>
<td>If one wants to have an output size as a ratio of the input size, this option can be given. This has to be a number or tuple in the range (0, 1)</td>
</tr>
<tr>
<td>return_indices</td>
<td>False</td>
<td>if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d .</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape = minibatch x channels x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h3 id="lppool2d">LPPool2d</h3>

<p>Applies a 2D power-average pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="c"># power-2 pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window of power 1.2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes.
On each window, the function computed is: f(X) = pow(sum(pow(X, p)), 1/p)
At p = infinity, one gets Max Pooling
At p = 1, one gets Average Pooling</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape = minibatch x channels x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

          <h2 id="non-linearities">Non-linearities</h2>

<h3 id="relu">ReLU</h3>

<p>Applies the rectified linear unit function element-wise ReLU(x)= max(0,x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/relu.png" ></p>

<h3 id="relu6">ReLU6</h3>

<p>Applies the element-wise function ReLU6(x) = min( max(0,x), 6)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/relu6.png" ></p>

<h3 id="threshold">Threshold</h3>

<p>Thresholds each element of the input Tensor</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>Threshold is defined as:
     y =  x        if x &gt;= threshold
          value    if x &lt;  threshold</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>threshold</td>
<td></td>
<td>The value to threshold at</td>
</tr>
<tr>
<td>value</td>
<td></td>
<td>The value to replace with</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>Tensor of same dimension and shape as the input</p>

<h3 id="hardtanh">Hardtanh</h3>

<p>Applies the HardTanh function element-wise</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HardTanh</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>HardTanh is defined as:
   f(x) = +1, if x  &gt;  1
   f(x) = -1, if x  &lt; -1
   f(x) =  x,  otherwise
The range of the linear region [-1, 1] can be adjusted</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>min_value</td>
<td></td>
<td>minimum value of the linear region range</td>
</tr>
<tr>
<td>max_value</td>
<td></td>
<td>maximum value of the linear region range</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/htanh.png" ></p>

<h3 id="sigmoid">Sigmoid</h3>

<p>Applies the element-wise function sigmoid(x) = 1 / ( 1 + exp(-x))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/sigmoid.png" ></p>

<h3 id="tanh">Tanh</h3>

<p>Applies element-wise, Tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/tanh.png" ></p>

<h3 id="elu">ELU</h3>

<p>Applies element-wise, ELU(x) = max(0,x) + min(0, alpha * (exp(x) - 1))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>alpha</td>
<td>1.0</td>
<td>the alpha value for the ELU formulation.</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/elu.png" ></p>

<h3 id="leakyrelu">LeakyReLU</h3>

<p>Applies element-wise, f(x) = max(0, x) + negative_slope * min(0, x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>negative_slope</td>
<td>1e-2</td>
<td>Controls the angle of the negative slope.</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="logsigmoid">LogSigmoid</h3>

<p>Applies element-wise LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/logsigmoid.png" ></p>

<h3 id="softplus">Softplus</h3>

<p>Applies element-wise SoftPlus(x) = 1/beta * log(1 + exp(beta * x_i))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.
For numerical stability the implementation reverts to the linear function
for inputs above a certain value.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>beta</td>
<td>1</td>
<td>the beta value for the Softplus formulation.</td>
</tr>
<tr>
<td>threshold</td>
<td>20</td>
<td>values above this revert to a linear function.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/softplus.png" ></p>

<h3 id="softshrink">Softshrink</h3>

<p>Applies the soft shrinkage function elementwise</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>SoftShrinkage operator is defined as:
    f(x) = x-lambda, if x &gt; lambda &gt;  f(x) = x+lambda, if x &lt; -lambda
    f(x) = 0, otherwise</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>lambd</td>
<td>0.5</td>
<td>the lambda value for the Softshrink formulation.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/sshrink.png" ></p>

<h3 id="prelu">PReLU</h3>

<p>Applies element-wise the function PReLU(x) = max(0,x) + a * min(0,x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>Here &ldquo;a&rdquo; is a learnable parameter.
When called without arguments, nn.PReLU() uses a single parameter &ldquo;a&rdquo;
across all input channels. If called with nn.PReLU(nChannels), a separate
&ldquo;a&rdquo; is used for each input channel.
Note that weight decay should not be used when learning &ldquo;a&rdquo; for good
performance.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_parameters</td>
<td>1</td>
<td>number of &ldquo;a&rdquo; to learn.</td>
</tr>
<tr>
<td>init</td>
<td>0.25</td>
<td>the initial value of &ldquo;a&rdquo;.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/prelu.png" ></p>

<h3 id="softsign">Softsign</h3>

<p>Applies element-wise, the function Softsign(x) = x / (1 + |x|)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/softsign.png" ></p>

<h3 id="tanhshrink">Tanhshrink</h3>

<p>Applies element-wise, Tanhshrink(x) = x - Tanh(x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanhshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="softmin">Softmin</h3>

<p>Applies the Softmin function to an n-dimensional input Tensor</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1
Softmin(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)
             where shift = max_i - x_i</p>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * ]</td>
<td>2D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input, with
    values in the range [0, 1]</p>

<p><img src="image/softmin.png" ></p>

<h3 id="softmax">Softmax</h3>

<p>Applies the Softmax function to an n-dimensional input Tensor</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1</p>

<p>Softmax is defined as f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)
                      where shift = max_i x_i</p>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * ]</td>
<td>2D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input with
    values in the range [0, 1]</p>

<p><img src="image/softmax.png" >
Notes:
    Note that this module doesn&rsquo;t work directly with NLLLoss,
    which expects the Log to be computed between the Softmax and itself.
    Use Logsoftmax instead (it&rsquo;s faster).</p>

<h3 id="softmax2d">Softmax2d</h3>

<p>Applies SoftMax over features to each spatial location</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax2d</span><span class="p">()</span>
<span class="c"># you softmax over the 2nd dimension</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>When given an image of Channels x Height x Width, it will
apply Softmax to each location [Channels, h_i, w_j]</p>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , * , * ]</td>
<td>4D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input with
    values in the range [0, 1]</p>

<h3 id="logsoftmax">LogSoftmax</h3>

<p>Applies the Log(Softmax(x)) function to an n-dimensional input Tensor.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>The LogSoftmax formulation can be simplified as
     f_i(x) = log(1 / a * exp(x_i)) where a = sum_j exp(x_j) .</p>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * ]</td>
<td>2D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a Tensor of the same dimension and shape as the input with
    values in the range [-inf, 0)</p>

<p><img src="image/logsoftmax.png" ></p>

          <h2 id="normalization-layers">Normalization layers</h2>

<h3 id="batchnorm1d">BatchNorm1d</h3>

<p>Applies Batch Normalization over a 2d input that is seen as a mini-batch of 1d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1
During evaluation, this running mean/variance is used for normalization.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>the size of each 1D input in the mini-batch</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features ]</td>
<td>2D Tensor of nBatches x num_features</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a normalized tensor in the batch dimension</p>

<h3 id="batchnorm2d">BatchNorm2d</h3>

<p>Applies Batch Normalization over a 4d input that is seen as a mini-batch of 3d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1
During evaluation, this running mean/variance is used for normalization.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>num_features from an expected input of size batch_size x num_features x height x width</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features , *, * ]</td>
<td>4D Tensor of batch_size x num_features x height x width</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a normalized tensor in the batch dimension</p>

<h3 id="batchnorm3d">BatchNorm3d</h3>

<p>Applies Batch Normalization over a 5d input that is seen as a mini-batch of 4d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1
During evaluation, this running mean/variance is used for normalization.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>num_features from an expected input of size batch_size x num_features x height x width</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features , * , * , * ]</td>
<td>5D Tensor of batch_size x num_features x depth x height x width</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a normalized tensor in the batch dimension</p>

          <h2 id="recurrent-layers">Recurrent layers</h2>

<h3 id="rnn">RNN</h3>

<p>Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.</p>
<pre class="highlight python"><code><span class="n">h_t</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">w_ih</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ih</span>  <span class="o">+</span>  <span class="n">w_hh</span> <span class="o">*</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hh</span><span class="p">)</span>
</code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</code></pre>

<p>For each element in the input sequence, each layer computes the following
function:
where <code class="prettyprint">h_t</code> is the hidden state at time t, and <code class="prettyprint">x_t</code> is the hidden
state of the previous layer at time t or <code class="prettyprint">input_t</code> for the first layer.
If nonlinearity=&lsquo;relu&rsquo;, then ReLU is used instead of tanh.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>num_layers</td>
<td></td>
<td>the size of the convolving kernel.</td>
</tr>
<tr>
<td>nonlinearity</td>
<td>&#39;tanh&rsquo;</td>
<td>The non-linearity to use [&#39;tanh&rsquo;</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
<tr>
<td>batch_first</td>
<td></td>
<td>If True, then the input tensor is provided as (batch, seq, feature)</td>
</tr>
<tr>
<td>dropout</td>
<td></td>
<td>If non-zero, introduces a dropout layer on the outputs of each RNN layer</td>
</tr>
<tr>
<td>bidirectional</td>
<td>False</td>
<td>If True, becomes a bidirectional RNN.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (seq_len x batch x input_size) tensor containing the features of the input sequence.</td>
</tr>
<tr>
<td>h_0</td>
<td></td>
<td>A (num_layers x batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>output</td>
<td>A (seq_len x batch x hidden_size) tensor containing the output features (h_k) from the last layer of the RNN, for each k</td>
</tr>
<tr>
<td>h_n</td>
<td>A (num_layers x batch x hidden_size) tensor containing the hidden state for k=seq_len</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih_l[k]</td>
<td>the learnable input-hidden weights of the k-th layer, of shape (input_size x hidden_size)</td>
</tr>
<tr>
<td>weight_hh_l[k]</td>
<td>the learnable hidden-hidden weights of the k-th layer, of shape (hidden_size x hidden_size)</td>
</tr>
<tr>
<td>bias_ih_l[k]</td>
<td>the learnable input-hidden bias of the k-th layer, of shape (hidden_size)</td>
</tr>
<tr>
<td>bias_hh_l[k]</td>
<td>the learnable hidden-hidden bias of the k-th layer, of shape (hidden_size)</td>
</tr>
</tbody></table>

<h3 id="lstm">LSTM</h3>

<p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p>
<pre class="highlight python"><code><span class="n">i_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ii</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ii</span> <span class="o">+</span> <span class="n">W_hi</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hi</span><span class="p">)</span>
<span class="n">f_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_if</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_if</span> <span class="o">+</span> <span class="n">W_hf</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hf</span><span class="p">)</span>
<span class="n">g_t</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">W_ig</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ig</span> <span class="o">+</span> <span class="n">W_hc</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hg</span><span class="p">)</span>
<span class="n">o_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_io</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_io</span> <span class="o">+</span> <span class="n">W_ho</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_ho</span><span class="p">)</span>
<span class="n">c_t</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">c_t</span>
<span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>
</code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">c0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
</code></pre>

<p>For each element in the input sequence, each layer computes the following
function:
where <code class="prettyprint">h_t</code> is the hidden state at time t, <code class="prettyprint">c_t</code> is the cell state at time t,
<code class="prettyprint">x_t</code> is the hidden state of the previous layer at time t or input_t for the first layer,
and <code class="prettyprint">i_t</code>, <code class="prettyprint">f_t</code>, <code class="prettyprint">g_t</code>, <code class="prettyprint">o_t</code> are the input, forget, cell, and out gates, respectively.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>num_layers</td>
<td></td>
<td>the size of the convolving kernel.</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
<tr>
<td>batch_first</td>
<td></td>
<td>If True, then the input tensor is provided as (batch, seq, feature)</td>
</tr>
<tr>
<td>dropout</td>
<td></td>
<td>If non-zero, introduces a dropout layer on the outputs of each RNN layer</td>
</tr>
<tr>
<td>bidirectional</td>
<td>False</td>
<td>If True, becomes a bidirectional RNN.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (seq_len x batch x input_size) tensor containing the features of the input sequence.</td>
</tr>
<tr>
<td>h_0</td>
<td></td>
<td>A (num_layers x batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
<tr>
<td>c_0</td>
<td></td>
<td>A (num_layers x batch x hidden_size) tensor containing the initial cell state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>output</td>
<td>A (seq_len x batch x hidden_size) tensor containing the output features (h_t) from the last layer of the RNN, for each t</td>
</tr>
<tr>
<td>h_n</td>
<td>A (num_layers x batch x hidden_size) tensor containing the hidden state for t=seq_len</td>
</tr>
<tr>
<td>c_n</td>
<td>A (num_layers x batch x hidden_size) tensor containing the cell state for t=seq_len</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih_l[k]</td>
<td>the learnable input-hidden weights of the k-th layer (W_ir</td>
</tr>
<tr>
<td>weight_hh_l[k]</td>
<td>the learnable hidden-hidden weights of the k-th layer (W_hr</td>
</tr>
<tr>
<td>bias_ih_l[k]</td>
<td>the learnable input-hidden bias of the k-th layer (b_ir</td>
</tr>
<tr>
<td>bias_hh_l[k]</td>
<td>the learnable hidden-hidden bias of the k-th layer (W_hr</td>
</tr>
</tbody></table>

<h3 id="gru">GRU</h3>

<p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p>
<pre class="highlight python"><code><span class="n">r_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ir</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ir</span> <span class="o">+</span> <span class="n">W_hr</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hr</span><span class="p">)</span>
<span class="n">i_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ii</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ii</span> <span class="o">+</span> <span class="n">W_hi</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hi</span><span class="p">)</span>
<span class="n">n_t</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">W_in</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">resetgate</span> <span class="o">*</span> <span class="n">W_hn</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">h_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">i_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_t</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</code></pre>

<p>For each element in the input sequence, each layer computes the following
function:
where <code class="prettyprint">h_t</code> is the hidden state at time t, <code class="prettyprint">x_t</code> is the hidden
state of the previous layer at time t or input_t for the first layer,
and <code class="prettyprint">r_t</code>, <code class="prettyprint">i_t</code>, <code class="prettyprint">n_t</code> are the reset, input, and new gates, respectively.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>num_layers</td>
<td></td>
<td>the size of the convolving kernel.</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
<tr>
<td>batch_first</td>
<td></td>
<td>If True, then the input tensor is provided as (batch, seq, feature)</td>
</tr>
<tr>
<td>dropout</td>
<td></td>
<td>If non-zero, introduces a dropout layer on the outputs of each RNN layer</td>
</tr>
<tr>
<td>bidirectional</td>
<td>False</td>
<td>If True, becomes a bidirectional RNN.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (seq_len x batch x input_size) tensor containing the features of the input sequence.</td>
</tr>
<tr>
<td>h_0</td>
<td></td>
<td>A (num_layers x batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>output</td>
<td>A (seq_len x batch x hidden_size) tensor containing the output features (h_t) from the last layer of the RNN, for each t</td>
</tr>
<tr>
<td>h_n</td>
<td>A (num_layers x batch x hidden_size) tensor containing the hidden state for t=seq_len</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih_l[k]</td>
<td>the learnable input-hidden weights of the k-th layer (W_ir</td>
</tr>
<tr>
<td>weight_hh_l[k]</td>
<td>the learnable hidden-hidden weights of the k-th layer (W_hr</td>
</tr>
<tr>
<td>bias_ih_l[k]</td>
<td>the learnable input-hidden bias of the k-th layer (b_ir</td>
</tr>
<tr>
<td>bias_hh_l[k]</td>
<td>the learnable hidden-hidden bias of the k-th layer (W_hr</td>
</tr>
</tbody></table>

<h3 id="rnncell">RNNCell</h3>

<p>An Elman RNN cell with tanh or ReLU non-linearity.</p>
<pre class="highlight python"><code><span class="n">h</span><span class="s">' = tanh(w_ih * x + b_ih  +  w_hh * h + b_hh)</span><span class="err">
</span></code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">hx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hx</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">hx</span>
</code></pre>

<p>If nonlinearity=&#39;relu&rsquo;, then ReLU is used in place of tanh.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
<tr>
<td>nonlinearity</td>
<td>&#39;tanh&rsquo;</td>
<td>The non-linearity to use [&#39;tanh&rsquo;</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (batch x input_size) tensor containing input features</td>
</tr>
<tr>
<td>hidden</td>
<td></td>
<td>A (batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>h&rsquo;</td>
<td>A (batch x hidden_size) tensor containing the next hidden state for each element in the batch</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih</td>
<td>the learnable input-hidden weights, of shape (input_size x hidden_size)</td>
</tr>
<tr>
<td>weight_hh</td>
<td>the learnable hidden-hidden weights, of shape (hidden_size x hidden_size)</td>
</tr>
<tr>
<td>bias_ih</td>
<td>the learnable input-hidden bias, of shape (hidden_size)</td>
</tr>
<tr>
<td>bias_hh</td>
<td>the learnable hidden-hidden bias, of shape (hidden_size)</td>
</tr>
</tbody></table>

<h3 id="lstmcell">LSTMCell</h3>

<p>A long short-term memory (LSTM) cell.</p>
<pre class="highlight python"><code><span class="n">i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ii</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_ii</span> <span class="o">+</span> <span class="n">W_hi</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hi</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_if</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_if</span> <span class="o">+</span> <span class="n">W_hf</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hf</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">W_ig</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_ig</span> <span class="o">+</span> <span class="n">W_hc</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hg</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_io</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_io</span> <span class="o">+</span> <span class="n">W_ho</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_ho</span><span class="p">)</span>
<span class="n">c</span><span class="s">' = f * c + i * c</span><span class="err">
</span><span class="s">h'</span> <span class="o">=</span> <span class="n">o</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>
</code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">cx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">hx</span><span class="p">,</span> <span class="n">cx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">hx</span><span class="p">,</span> <span class="n">cx</span><span class="p">))</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">hx</span>
</code></pre>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (batch x input_size) tensor containing input features</td>
</tr>
<tr>
<td>hidden</td>
<td></td>
<td>A (batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>h&rsquo;</td>
<td>A (batch x hidden_size) tensor containing the next hidden state for each element in the batch</td>
</tr>
<tr>
<td>c&rsquo;</td>
<td>A (batch x hidden_size) tensor containing the next cell state for each element in the batch</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih</td>
<td>the learnable input-hidden weights, of shape (input_size x hidden_size)</td>
</tr>
<tr>
<td>weight_hh</td>
<td>the learnable hidden-hidden weights, of shape (hidden_size x hidden_size)</td>
</tr>
<tr>
<td>bias_ih</td>
<td>the learnable input-hidden bias, of shape (hidden_size)</td>
</tr>
<tr>
<td>bias_hh</td>
<td>the learnable hidden-hidden bias, of shape (hidden_size)</td>
</tr>
</tbody></table>

<h3 id="grucell">GRUCell</h3>

<p>A gated recurrent unit (GRU) cell</p>
<pre class="highlight python"><code><span class="n">r</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ir</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_ir</span> <span class="o">+</span> <span class="n">W_hr</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hr</span><span class="p">)</span>
<span class="n">i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ii</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_ii</span> <span class="o">+</span> <span class="n">W_hi</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hi</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">W_in</span> <span class="n">x</span> <span class="o">+</span> <span class="n">resetgate</span> <span class="o">*</span> <span class="n">W_hn</span> <span class="n">h</span><span class="p">)</span>
<span class="n">h</span><span class="s">' = (1 - i) * n + i * h</span><span class="err">
</span></code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">hx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hx</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">hx</span>
</code></pre>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (batch x input_size) tensor containing input features</td>
</tr>
<tr>
<td>hidden</td>
<td></td>
<td>A (batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>h&rsquo;</td>
<td>A (batch x hidden_size) tensor containing the next hidden state for each element in the batch</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih</td>
<td>the learnable input-hidden weights, of shape (input_size x hidden_size)</td>
</tr>
<tr>
<td>weight_hh</td>
<td>the learnable hidden-hidden weights, of shape (hidden_size x hidden_size)</td>
</tr>
<tr>
<td>bias_ih</td>
<td>the learnable input-hidden bias, of shape (hidden_size)</td>
</tr>
<tr>
<td>bias_hh</td>
<td>the learnable hidden-hidden bias, of shape (hidden_size)</td>
</tr>
</tbody></table>

          <h2 id="linear-layers">Linear layers</h2>

<h3 id="linear">Linear</h3>

<p>Applies a linear transformation to the incoming data, y = Ax + b</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre>

<p>The input is a 2D mini-batch of samples, each of size in_features
The output will be a 2D Tensor of size mini-batch x out_features</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_features</td>
<td></td>
<td>size of each input sample</td>
</tr>
<tr>
<td>out_features</td>
<td></td>
<td>size of each output sample</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If set to False, the layer will not learn an additive bias.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[*, in_features]</td>
<td>Input can be of shape minibatch x in_features</td>
</tr>
<tr>
<td>output</td>
<td>[*, out_features]</td>
<td>Output is of shape minibatch x out_features</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>the learnable weights of the module of shape (out_features x in_features)</td>
</tr>
<tr>
<td>bias</td>
<td>the learnable bias of the module of shape (out_features)</td>
</tr>
</tbody></table>

          <h2 id="dropout-layers">Dropout layers</h2>

<h3 id="dropout">Dropout</h3>

<p>Randomly zeroes some of the elements of the input tensor.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The elements to zero are randomized on every forward call.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>p</td>
<td>0.5</td>
<td>probability of an element to be zeroed.</td>
</tr>
<tr>
<td>inplace</td>
<td>false</td>
<td>If set to True, will do this operation in-place.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Input can be of any shape</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output is of the same shape as input</td>
</tr>
</tbody></table>

<h3 id="dropout2d">Dropout2d</h3>

<p>Randomly zeroes whole channels of the input tensor.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The input is 4D (batch x channels, height, width) and each channel
is of size (1, height, width).
The channels to zero are randomized on every forward call.
Usually the input comes from Conv2d modules.</p>

<p>As described in the paper &ldquo;Efficient Object Localization Using Convolutional
Networks&rdquo; (http:arxiv.org/abs/1411.4280), if adjacent pixels within
feature maps are strongly correlated (as is normally the case in early
convolution layers) then iid dropout will not regularize the activations
and will otherwise just result in an effective learning rate decrease.
In this case, nn.Dropout2d will help promote independence between
feature maps and should be used instead.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>p</td>
<td>0.5</td>
<td>probability of an element to be zeroed.</td>
</tr>
<tr>
<td>inplace</td>
<td>false</td>
<td>If set to True, will do this operation in-place.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[*, *, *, *]</td>
<td>Input can be of any sizes of 4D shape</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output is of the same shape as input</td>
</tr>
</tbody></table>

<h3 id="dropout3d">Dropout3d</h3>

<p>Randomly zeroes whole channels of the input tensor.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout3d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The input is 5D (batch x channels, depth, height, width) and each channel
is of size (1, depth, height, width).
The channels to zero are randomized on every forward call.
Usually the input comes from Conv3d modules.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>p</td>
<td>0.5</td>
<td>probability of an element to be zeroed.</td>
</tr>
<tr>
<td>inplace</td>
<td>false</td>
<td>If set to True, will do this operation in-place.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[*, *, *, *, *]</td>
<td>Input can be of any sizes of 5D shape</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output is of the same shape as input</td>
</tr>
</tbody></table>

          <h2 id="sparse-layers">Sparse layers</h2>

<h3 id="embedding">Embedding</h3>

<p>A simple lookup table that stores embeddings of a fixed dictionary and size</p>
<pre class="highlight python"><code><span class="c"># an Embedding module containing 10 tensors of size 3</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c"># a batch of 2 samples of 4 indices each</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
<span class="c"># example with padding_idx</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_embeddings</td>
<td></td>
<td>size of the dictionary of embeddings</td>
</tr>
<tr>
<td>embedding_dim</td>
<td></td>
<td>the size of each embedding vector</td>
</tr>
<tr>
<td>padding_idx</td>
<td>None</td>
<td>If given, pads the output with zeros whenever it encounters the index.</td>
</tr>
<tr>
<td>max_norm</td>
<td>None</td>
<td>If given, will renormalize the embeddings to always have a norm lesser than this</td>
</tr>
<tr>
<td>norm_type</td>
<td></td>
<td>The p of the p-norm to compute for the max_norm option</td>
</tr>
<tr>
<td>scale_grad_by_freq</td>
<td></td>
<td>if given, this will scale gradients by the frequency of the words in the dictionary.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ *, * ]</td>
<td>Input is a 2D mini_batch LongTensor of m x n indices to extract from the Embedding dictionary</td>
</tr>
<tr>
<td>output</td>
<td>[ * , *, * ]</td>
<td>Output shape = m x n x embedding_dim</td>
</tr>
</tbody></table>

          <h2 id="loss-functions">Loss functions</h2>

<h3 id="l1loss">L1Loss</h3>

<p>Creates a criterion that measures the mean absolute value of the </p>

<p>element-wise difference between input <code class="prettyprint">x</code> and target <code class="prettyprint">y</code>:</p>

<p>loss(x, y)  = 1/n \sum |x_i - y_i|</p>

<p><code class="prettyprint">x</code> and <code class="prettyprint">y</code> arbitrary shapes with a total of <code class="prettyprint">n</code> elements each
the sum operation still operates over all the elements, and divides by <code class="prettyprint">n</code>.</p>

<p>The division by <code class="prettyprint">n</code> can be avoided if one sets the internal 
variable <code class="prettyprint">sizeAverage</code> to <code class="prettyprint">False</code></p>

<h3 id="mseloss">MSELoss</h3>

<p>Creates a criterion that measures the mean squared error between </p>

<p><code class="prettyprint">n</code> elements in the input <code class="prettyprint">x</code> and target <code class="prettyprint">y</code>:
    loss(x, y) = 1/n \sum |x_i - y_i|^2
<code class="prettyprint">x</code> and <code class="prettyprint">y</code> arbitrary shapes with a total of <code class="prettyprint">n</code> elements each
the sum operation still operates over all the elements, and divides by <code class="prettyprint">n</code>.</p>

<p>The division by <code class="prettyprint">n</code> can be avoided if one sets the internal variable 
<code class="prettyprint">sizeAverage</code> to <code class="prettyprint">False</code>
By default, the losses are averaged over observations for each minibatch. 
However, if the field <code class="prettyprint">sizeAverage = False</code>, the losses are instead summed.</p>

<h3 id="crossentropyloss">CrossEntropyLoss</h3>

<p>This criterion combines <code class="prettyprint">LogSoftMax</code> and <code class="prettyprint">ClassNLLLoss</code> in one single class.</p>

<p>It is useful when training a classification problem with <code class="prettyprint">n</code> classes.
If provided, the optional argument <code class="prettyprint">weights</code> should be a 1D <code class="prettyprint">Tensor</code> 
assigning weight to each of the classes. 
This is particularly useful when you have an unbalanced training set.</p>

<p>The <code class="prettyprint">input</code> is expected to contain scores for each class: 
      <code class="prettyprint">input</code> has to be a 2D <code class="prettyprint">Tensor</code> of size <code class="prettyprint">batch x n</code>.
This criterion expects a class index (0 to nClasses-1) as the 
<code class="prettyprint">target</code> for each value of a 1D tensor of size <code class="prettyprint">n</code></p>

<p>The loss can be described as:</p>

<p>loss(x, class) = -log(exp(x[class]) / (\sum_j exp(x[j])))
               = -x[class] + log(\sum_j exp(x[j]))</p>

<p>or in the case of the <code class="prettyprint">weights</code> argument being specified:</p>

<p>loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))</p>

<p>The losses are averaged across observations for each minibatch.</p>

<h3 id="nllloss">NLLLoss</h3>

<p>The negative log likelihood loss. It is useful to train a classication problem with n classes</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="c"># input is of size nBatch x nClasses = 3 x 5</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="c"># each element in target has to have 0 &lt;= value &lt; nclasses </span>
<span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre>

<p>If provided, the optional argument <code class="prettyprint">weights</code> should be a 1D Tensor assigning
weight to each of the classes.
This is particularly useful when you have an unbalanced training set.</p>

<p>The input given through a forward call is expected to contain log-probabilities
of each class: input has to be a 2D Tensor of size minibatch x n
Obtaining log-probabilities in a neural network is easily achieved by
adding a  <code class="prettyprint">LogSoftmax</code>  layer in the last layer.
You may use <code class="prettyprint">CrossEntropyLoss</code>  instead, if you prefer not to
add an extra layer.</p>

<p>The target that this loss expects is a class index (1 to the number of class)</p>

<p>The loss can be described as:
    loss(x, class) = -x[class]</p>

<p>or in the case of the weights argument it is specified as follows:
    loss(x, class) = -weights[class] * x[class]</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>None</td>
<td>a manual rescaling weight given to each class. If given, has to be a Tensor of size &ldquo;nclasses&rdquo;.</td>
</tr>
<tr>
<td>size_average</td>
<td>True</td>
<td>By default, the losses are averaged over observations for each minibatch. However, if the field sizeAverage is set to False, the losses are instead summed for each minibatch.</td>
</tr>
</tbody></table>

<p>Target Shape: [ * ] : Targets of size [minibatch], each value has to be 1 &lt;= targets[i] &lt;= nClasses</p>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight</td>
<td>the class-weights given as input to the constructor</td>
</tr>
</tbody></table>

<h3 id="nllloss2d">NLLLoss2d</h3>

<p>This is negative log likehood loss, but for image inputs. It computes NLL loss per-pixel.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss2d</span><span class="p">()</span>
<span class="c"># input is of size nBatch x nClasses x height x width</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="c"># each element in target has to have 0 &lt;= value &lt; nclasses</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre>

<p>This loss does not support per-class weights</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>size_average</td>
<td>True</td>
<td>By default, the losses are averaged over observations for each minibatch. However, if the field sizeAverage is set to False, the losses are instead summed for each minibatch.</td>
</tr>
</tbody></table>

<p>Target Shape: [ * , *, *] : Targets of size minibatch x height x width, each value has to be 1 &lt;= targets[i] &lt;= nClasses</p>

<h3 id="kldivloss">KLDivLoss</h3>

<p>The <a href="http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a> Loss</p>

<p>KL divergence is a useful distance measure for continuous distributions 
and is often useful when performing direct regression over the space of
(discretely sampled) continuous output distributions.
As with ClassNLLLoss, the <code class="prettyprint">input</code> given is expected to contain 
<em>log-probabilities</em>, however unlike ClassNLLLoss, <code class="prettyprint">input</code> is not 
restricted to a 2D Tensor, because the criterion is applied element-wise.</p>

<p>This criterion expects a <code class="prettyprint">target</code> <code class="prettyprint">Tensor</code> of the same size as the 
<code class="prettyprint">input</code> <code class="prettyprint">Tensor</code>.</p>

<p>The loss can be described as:
    loss(x, target) = 1/n \sum(target_i * (log(target_i) - x_i))</p>

<p>By default, the losses are averaged for each minibatch over observations 
<em>as well as</em> over dimensions. However, if the field 
<code class="prettyprint">sizeAverage</code> is set to <code class="prettyprint">False</code>, the losses are instead summed.</p>

<h3 id="bceloss">BCELoss</h3>

<p>Creates a criterion that measures the Binary Cross Entropy </p>

<p>between the target and the output:
    loss(o, t) = - 1/n sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))</p>

<p>or in the case of the weights argument being specified:
    loss(o, t) = - 1/n sum_i weights[i] * (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))</p>

<p>This is used for measuring the error of a reconstruction in for example 
an auto-encoder. Note that the targets <code class="prettyprint">t[i]</code> should be numbers between 0 and 1, 
for instance, the output of an <code class="prettyprint">nn.Sigmoid</code> layer.</p>

<p>By default, the losses are averaged for each minibatch over observations 
<em>as well as</em> over dimensions. However, if the field <code class="prettyprint">sizeAverage</code> is set 
to <code class="prettyprint">False</code>, the losses are instead summed.</p>

<h3 id="marginrankingloss">MarginRankingLoss</h3>

<p>Creates a criterion that measures the loss given  </p>

<p>inputs <code class="prettyprint">x1</code>, <code class="prettyprint">x2</code>, two 1D min-batch <code class="prettyprint">Tensor</code>s, 
and a label 1D mini-batch tensor <code class="prettyprint">y</code> with values (<code class="prettyprint">1</code> or <code class="prettyprint">-1</code>).</p>

<p>If <code class="prettyprint">y == 1</code> then it assumed the first input should be ranked higher 
(have a larger value) than the second input, and vice-versa for <code class="prettyprint">y == -1</code>.</p>

<p>The loss function for each sample in the mini-batch is:</p>

<p>loss(x, y) = max(0, -y * (x1 - x2) + margin)</p>

<p>if the internal variable <code class="prettyprint">sizeAverage = True</code>, 
the loss function averages the loss over the batch samples; 
if <code class="prettyprint">sizeAverage = False</code>, then the loss function sums over the batch samples. 
By default, <code class="prettyprint">sizeAverage</code> equals to <code class="prettyprint">True</code>.</p>

<h3 id="hingeembeddingloss">HingeEmbeddingLoss</h3>

<p>Measures the loss given an input <code class="prettyprint">x</code> which is a 2D mini-batch tensor</p>

<p>and a labels <code class="prettyprint">y</code>, a 1D tensor containg values (<code class="prettyprint">1</code> or <code class="prettyprint">-1</code>).
This is usually used for measuring whether two inputs are similar or dissimilar, 
e.g. using the L1 pairwise distance, and is typically used for learning 
nonlinear embeddings or semi-supervised learning.</p>

<p>{ x_i,                  if y_i ==  1
    loss(x, y) = 1/n {
                     { max(0, margin - x_i), if y_i == -1</p>

<p><code class="prettyprint">x</code> and <code class="prettyprint">y</code> arbitrary shapes with a total of <code class="prettyprint">n</code> elements each
the sum operation still operates over all the elements, and divides by <code class="prettyprint">n</code>.
(the division by <code class="prettyprint">n</code> can be avoided if one sets the internal variable <code class="prettyprint">sizeAverage=False</code>). 
The <code class="prettyprint">margin</code> has a default value of <code class="prettyprint">1</code>, or can be set in the constructor.</p>

<h3 id="multilabelmarginloss">MultiLabelMarginLoss</h3>

<p>Creates a criterion that optimizes a multi-class multi-classification </p>

<p>hinge loss (margin-based loss) between input <code class="prettyprint">x</code>  (a 2D mini-batch <code class="prettyprint">Tensor</code>) and 
output <code class="prettyprint">y</code> (which is a 2D <code class="prettyprint">Tensor</code> of target class indices).
For each sample in the mini-batch:</p>

<p>loss(x, y) = sum_ij(max(0, 1 - (x[y[j]] - x[i]))) / x:size(1)</p>

<p>where <code class="prettyprint">i == 0</code> to <code class="prettyprint">x.size(0)</code>, <code class="prettyprint">j == 0</code> to <code class="prettyprint">y.size(0)</code>, 
      <code class="prettyprint">y[j] != 0</code>, and <code class="prettyprint">i != y[j]</code> for all <code class="prettyprint">i</code> and <code class="prettyprint">j</code>.</p>

<p><code class="prettyprint">y</code> and <code class="prettyprint">x</code> must have the same size.
The criterion only considers the first non zero <code class="prettyprint">y[j]</code> targets.
This allows for different samples to have variable amounts of target classes</p>

<h3 id="smoothl1loss">SmoothL1Loss</h3>

<p>Creates a criterion that uses a squared term if the absolute </p>

<p>element-wise error falls below 1 and an L1 term otherwise. 
It is less sensitive to outliers than the <code class="prettyprint">MSELoss</code> and in some cases 
prevents exploding gradients (e.g. see &ldquo;Fast R-CNN&rdquo; paper by Ross Girshick).
Also known as the Huber loss.</p>

<p>{ 0.5 * (x_i - y_i)^2, if |x_i - y_i| &lt; 1
    loss(x, y) = 1/n \sum {
                          { |x_i - y_i| - 0.5,   otherwise</p>

<p><code class="prettyprint">x</code> and <code class="prettyprint">y</code> arbitrary shapes with a total of <code class="prettyprint">n</code> elements each
the sum operation still operates over all the elements, and divides by <code class="prettyprint">n</code>.</p>

<p>The division by <code class="prettyprint">n</code> can be avoided if one sets the internal variable 
<code class="prettyprint">sizeAverage</code> to <code class="prettyprint">False</code></p>

<h3 id="softmarginloss">SoftMarginLoss</h3>

<p>Creates a criterion that optimizes a two-class classification </p>

<p>logistic loss between input <code class="prettyprint">x</code> (a 2D mini-batch <code class="prettyprint">Tensor</code>) and 
target <code class="prettyprint">y</code> (which is a tensor containing either <code class="prettyprint">1</code>s or <code class="prettyprint">-1</code>s).</p>

<p>loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x:nElement()</p>

<p>The normalization by the number of elements in the input can be disabled by
setting <code class="prettyprint">self.sizeAverage</code> to <code class="prettyprint">False</code>.</p>

<h3 id="multilabelsoftmarginloss">MultiLabelSoftMarginLoss</h3>

<p>Creates a criterion that optimizes a multi-label one-versus-all </p>

<p>loss based on max-entropy, between input <code class="prettyprint">x</code>  (a 2D mini-batch <code class="prettyprint">Tensor</code>) and 
target <code class="prettyprint">y</code> (a binary 2D <code class="prettyprint">Tensor</code>). For each sample in the minibatch:</p>

<p>loss(x, y) = - sum_i (y[i] log( exp(x[i]) / (1 + exp(x[i]))) 
                         + (1-y[i]) log(1/(1+exp(x[i])))) / x:nElement()</p>

<p>where <code class="prettyprint">i == 0</code> to <code class="prettyprint">x.nElement()-1</code>, <code class="prettyprint">y[i]  in {0,1}</code>.
<code class="prettyprint">y</code> and <code class="prettyprint">x</code> must have the same size.</p>

<h3 id="cosineembeddingloss">CosineEmbeddingLoss</h3>

<p>Creates a criterion that measures the loss given  an input tensors x1, x2 </p>

<p>and a <code class="prettyprint">Tensor</code> label <code class="prettyprint">y</code> with values 1 or -1.
This is used for measuring whether two inputs are similar or dissimilar, 
using the cosine distance, and is typically used for learning nonlinear 
embeddings or semi-supervised learning.</p>

<p><code class="prettyprint">margin</code> should be a number from <code class="prettyprint">-1</code> to <code class="prettyprint">1</code>, <code class="prettyprint">0</code> to <code class="prettyprint">0.5</code> is suggested.
If <code class="prettyprint">margin</code> is missing, the default value is <code class="prettyprint">0</code>.</p>

<p>The loss function for each sample is:</p>

<p>{ 1 - cos(x1, x2),              if y ==  1
    loss(x, y) = {
                 { max(0, cos(x1, x2) - margin), if y == -1</p>

<p>If the internal variable <code class="prettyprint">sizeAverage</code> is equal to <code class="prettyprint">True</code>, 
the loss function averages the loss over the batch samples; 
if <code class="prettyprint">sizeAverage</code> is <code class="prettyprint">False</code>, then the loss function sums over the 
batch samples. By default, <code class="prettyprint">sizeAverage = True</code>.</p>

<h3 id="multimarginloss">MultiMarginLoss</h3>

<p>Creates a criterion that optimizes a multi-class classification hinge loss </p>

<p>(margin-based loss) between input <code class="prettyprint">x</code> (a 2D mini-batch <code class="prettyprint">Tensor</code>) and 
output <code class="prettyprint">y</code> (which is a 1D tensor of target class indices, <code class="prettyprint">0</code> &lt;= <code class="prettyprint">y</code> &lt;= <code class="prettyprint">x.size(1)</code>):</p>

<p>For each mini-batch sample:
    loss(x, y) = sum_i(max(0, (margin - x[y] + x[i]))^p) / x.size(0)
                 where <code class="prettyprint">i == 0</code> to <code class="prettyprint">x.size(0)</code> and <code class="prettyprint">i != y</code>.</p>

<p>Optionally, you can give non-equal weighting on the classes by passing 
a 1D <code class="prettyprint">weights</code> tensor into the constructor.</p>

<p>The loss function then becomes:
    loss(x, y) = sum_i(max(0, w[y] * (margin - x[y] - x[i]))^p) / x.size(0)</p>

<p>By default, the losses are averaged over observations for each minibatch. 
However, if the field <code class="prettyprint">sizeAverage</code> is set to <code class="prettyprint">False</code>, 
the losses are instead summed.</p>

          <h1 id="torch-optim">torch.optim</h1>

<p>The Optim package in Torch is targeted for one to optimize their neural networks
using a wide variety of optimization methods such as SGD, Adam etc.</p>

<p>Currently, the following optimization methods are supported, typically with
options such as weight decay and other bells and whistles.</p>

<ul>
<li>SGD          <code class="prettyprint">(params, lr=required, momentum=0, dampening=0)</code></li>
<li>AdaDelta     <code class="prettyprint">(params, rho=0.9, eps=1e-6, weight_decay=0)</code></li>
<li>Adagrad      <code class="prettyprint">(params, lr=1e-2, lr_decay=0, weight_decay=0)</code></li>
<li>Adam         <code class="prettyprint">(params, lr=1e-2, betas=(0.9, 0.999), epsilon=1e-8, weight_decay=0)</code></li>
<li>AdaMax       <code class="prettyprint">(params, lr=1e-2, betas=(0.9, 0.999), eps=1e-38, weight_decay=0)</code></li>
<li>Averaged SGD <code class="prettyprint">(params, lr=1e-2, lambd=1e-4, alpha=0.75, t0=1e6, weight_decay=0)</code></li>
<li>RProp        <code class="prettyprint">(params, lr=1e-2, etas=(0.5, 1.2), step_sizes=(1e-6, 50))</code></li>
<li>RMSProp      <code class="prettyprint">(params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0)</code></li>
</ul>

<p>The usage of the Optim package itself is as follows.</p>

<ol>
<li>Construct an optimizer</li>
<li>Use <code class="prettyprint">optimizer.step(...)</code> to optimize.

<ul>
<li>Call <code class="prettyprint">optimizer.zero_grad()</code> to zero out the gradient buffers when appropriate</li>
</ul></li>
</ol>

<h2 id="1-constructing-the-optimizer">1. Constructing the optimizer</h2>

<p>One first constructs an <code class="prettyprint">Optimizer</code> object by giving it a list of parameters
to optimize, as well as the optimizer options,such as learning rate, weight decay, etc.</p>

<p>Examples:</p>

<p><code class="prettyprint">optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)</code></p>

<p><code class="prettyprint">optimizer = optim.Adam([var1, var2], lr = 0.0001)</code></p>

<h3 id="per-parameter-options">Per-parameter options</h3>

<p>In a more advanced usage, one can specify per-layer options by passing each parameter group along with it&rsquo;s custom options.</p>

<p><strong><strong>Any parameter group that does not have an attribute defined will use the default attributes.</strong></strong></p>

<p>This is very useful when one wants to specify per-layer learning rates for example.</p>

<p>Example:</p>

<p><code class="prettyprint">optim.SGD([{&#39;params&#39;: model1.parameters()}, {&#39;params&#39;: model2.parameters(), &#39;lr&#39;: 1e-3}, lr=1e-2, momentum=0.9)</code></p>

<p><code class="prettyprint">model1</code>&rsquo;s parameters will use the default learning rate of <code class="prettyprint">1e-2</code> and momentum of <code class="prettyprint">0.9</code>
<code class="prettyprint">model2</code>&rsquo;s parameters will use a learning rate of <code class="prettyprint">1e-3</code>, and the default momentum of <code class="prettyprint">0.9</code></p>

<p>Then, you can use the optimizer by calling <code class="prettyprint">optimizer.zero_grad()</code> and <code class="prettyprint">optimizer.step(...)</code>. Read the next sections.</p>

<h2 id="2-taking-an-optimization-step-using-optimizer-step">2. Taking an optimization step using <code class="prettyprint">Optimizer.step(...)</code></h2>

<p>The step function has the following two signatures:</p>

<h3 id="a-optimizer-step-closure">a. <code class="prettyprint">Optimizer.step(closure)</code></h3>

<p>The <code class="prettyprint">step</code> function takes a user-defined closure that computes f(x) and returns the loss.</p>

<p>The closure needs to do the following:
- Optimizer.zero_grad()
- Compute the loss
- Call loss.backward()
- return the loss</p>

<p>Example 1: training a neural network</p>
<pre class="highlight python"><code><span class="c"># Example 1: training a neural network with optimizer.step(closure)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MNISTNet</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">ClassNLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_batches</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">def</span> <span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
</code></pre>

<p>Notes: Why is this required? Why cant we simply have the optimizer take the parameters and grads?
       Some optimization algorithms such as Conjugate Gradient and LBFGS need to evaluate their function
       multiple times. For such optimization methods, the function (i.e. the closure) has to be defined.</p>

<h3 id="b-optimizer-step">b. <code class="prettyprint">Optimizer.step()</code></h3>

<p>This is a simplified usage that supports most, but not all optimization algorithms. For example, it does not support LBFGS or Conjugate Gradient.</p>

<p>The usage for this is to simply call the function after the backward() is called on your model.</p>

<p>Example 2: training a neural network</p>
<pre class="highlight python"><code><span class="c"># Example 2: training a neural network with optimizer.step()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">MNISTNet</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">ClassNLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_batches</span><span class="p">:</span>
    <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre>

      </div>
      <div class="dark-box">
          <div class="lang-selector">
          </div>
      </div>
    </div>
  </body>
</html>
