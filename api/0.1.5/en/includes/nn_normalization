<h2 id="normalization-layers">Normalization layers</h2>

<h3 id="batchnorm1d">BatchNorm1d</h3>

<p>Applies Batch Normalization over a 2d input that is seen as a mini-batch of 1d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1
During evaluation, this running mean/variance is used for normalization.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>the size of each 1D input in the mini-batch</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features ]</td>
<td>2D Tensor of nBatches x num_features</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a normalized tensor in the batch dimension</p>

<h3 id="batchnorm2d">BatchNorm2d</h3>

<p>Applies Batch Normalization over a 4d input that is seen as a mini-batch of 3d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1
During evaluation, this running mean/variance is used for normalization.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>num_features from an expected input of size batch_size x num_features x height x width</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features , *, * ]</td>
<td>4D Tensor of batch_size x num_features x height x width</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a normalized tensor in the batch dimension</p>

<h3 id="batchnorm3d">BatchNorm3d</h3>

<p>Applies Batch Normalization over a 5d input that is seen as a mini-batch of 4d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1
During evaluation, this running mean/variance is used for normalization.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>num_features from an expected input of size batch_size x num_features x height x width</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h4 id="expected-shape">Expected Shape</h4>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features , * , * , * ]</td>
<td>5D Tensor of batch_size x num_features x depth x height x width</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h4 id="returns">Returns</h4>

<p>a normalized tensor in the batch dimension</p>
