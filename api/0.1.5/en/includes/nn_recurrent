<h2 id="recurrent-layers">Recurrent layers</h2>

<h3 id="rnn">RNN</h3>

<p>Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.</p>
<pre class="highlight python"><code><span class="n">h_t</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">w_ih</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ih</span>  <span class="o">+</span>  <span class="n">w_hh</span> <span class="o">*</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hh</span><span class="p">)</span>
</code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</code></pre>

<p>For each element in the input sequence, each layer computes the following
function:
where <code class="prettyprint">h_t</code> is the hidden state at time t, and <code class="prettyprint">x_t</code> is the hidden
state of the previous layer at time t or <code class="prettyprint">input_t</code> for the first layer.
If nonlinearity=&lsquo;relu&rsquo;, then ReLU is used instead of tanh.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>num_layers</td>
<td></td>
<td>the size of the convolving kernel.</td>
</tr>
<tr>
<td>nonlinearity</td>
<td>&#39;tanh&rsquo;</td>
<td>The non-linearity to use [&#39;tanh&rsquo;</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
<tr>
<td>batch_first</td>
<td></td>
<td>If True, then the input tensor is provided as (batch, seq, feature)</td>
</tr>
<tr>
<td>dropout</td>
<td></td>
<td>If non-zero, introduces a dropout layer on the outputs of each RNN layer</td>
</tr>
<tr>
<td>bidirectional</td>
<td>False</td>
<td>If True, becomes a bidirectional RNN.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (seq_len x batch x input_size) tensor containing the features of the input sequence.</td>
</tr>
<tr>
<td>h_0</td>
<td></td>
<td>A (num_layers x batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>output</td>
<td>A (seq_len x batch x hidden_size) tensor containing the output features (h_k) from the last layer of the RNN, for each k</td>
</tr>
<tr>
<td>h_n</td>
<td>A (num_layers x batch x hidden_size) tensor containing the hidden state for k=seq_len</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih_l[k]</td>
<td>the learnable input-hidden weights of the k-th layer, of shape (input_size x hidden_size)</td>
</tr>
<tr>
<td>weight_hh_l[k]</td>
<td>the learnable hidden-hidden weights of the k-th layer, of shape (hidden_size x hidden_size)</td>
</tr>
<tr>
<td>bias_ih_l[k]</td>
<td>the learnable input-hidden bias of the k-th layer, of shape (hidden_size)</td>
</tr>
<tr>
<td>bias_hh_l[k]</td>
<td>the learnable hidden-hidden bias of the k-th layer, of shape (hidden_size)</td>
</tr>
</tbody></table>

<h3 id="lstm">LSTM</h3>

<p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p>
<pre class="highlight python"><code><span class="n">i_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ii</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ii</span> <span class="o">+</span> <span class="n">W_hi</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hi</span><span class="p">)</span>
<span class="n">f_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_if</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_if</span> <span class="o">+</span> <span class="n">W_hf</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hf</span><span class="p">)</span>
<span class="n">g_t</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">W_ig</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ig</span> <span class="o">+</span> <span class="n">W_hc</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hg</span><span class="p">)</span>
<span class="n">o_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_io</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_io</span> <span class="o">+</span> <span class="n">W_ho</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_ho</span><span class="p">)</span>
<span class="n">c_t</span> <span class="o">=</span> <span class="n">f_t</span> <span class="o">*</span> <span class="n">c_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">c_t</span>
<span class="n">h_t</span> <span class="o">=</span> <span class="n">o_t</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>
</code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">c0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
</code></pre>

<p>For each element in the input sequence, each layer computes the following
function:
where <code class="prettyprint">h_t</code> is the hidden state at time t, <code class="prettyprint">c_t</code> is the cell state at time t,
<code class="prettyprint">x_t</code> is the hidden state of the previous layer at time t or input_t for the first layer,
and <code class="prettyprint">i_t</code>, <code class="prettyprint">f_t</code>, <code class="prettyprint">g_t</code>, <code class="prettyprint">o_t</code> are the input, forget, cell, and out gates, respectively.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>num_layers</td>
<td></td>
<td>the size of the convolving kernel.</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
<tr>
<td>batch_first</td>
<td></td>
<td>If True, then the input tensor is provided as (batch, seq, feature)</td>
</tr>
<tr>
<td>dropout</td>
<td></td>
<td>If non-zero, introduces a dropout layer on the outputs of each RNN layer</td>
</tr>
<tr>
<td>bidirectional</td>
<td>False</td>
<td>If True, becomes a bidirectional RNN.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (seq_len x batch x input_size) tensor containing the features of the input sequence.</td>
</tr>
<tr>
<td>h_0</td>
<td></td>
<td>A (num_layers x batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
<tr>
<td>c_0</td>
<td></td>
<td>A (num_layers x batch x hidden_size) tensor containing the initial cell state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>output</td>
<td>A (seq_len x batch x hidden_size) tensor containing the output features (h_t) from the last layer of the RNN, for each t</td>
</tr>
<tr>
<td>h_n</td>
<td>A (num_layers x batch x hidden_size) tensor containing the hidden state for t=seq_len</td>
</tr>
<tr>
<td>c_n</td>
<td>A (num_layers x batch x hidden_size) tensor containing the cell state for t=seq_len</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih_l[k]</td>
<td>the learnable input-hidden weights of the k-th layer (W_ir</td>
</tr>
<tr>
<td>weight_hh_l[k]</td>
<td>the learnable hidden-hidden weights of the k-th layer (W_hr</td>
</tr>
<tr>
<td>bias_ih_l[k]</td>
<td>the learnable input-hidden bias of the k-th layer (b_ir</td>
</tr>
<tr>
<td>bias_hh_l[k]</td>
<td>the learnable hidden-hidden bias of the k-th layer (W_hr</td>
</tr>
</tbody></table>

<h3 id="gru">GRU</h3>

<p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p>
<pre class="highlight python"><code><span class="n">r_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ir</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ir</span> <span class="o">+</span> <span class="n">W_hr</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hr</span><span class="p">)</span>
<span class="n">i_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ii</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">b_ii</span> <span class="o">+</span> <span class="n">W_hi</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_hi</span><span class="p">)</span>
<span class="n">n_t</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">W_in</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">resetgate</span> <span class="o">*</span> <span class="n">W_hn</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">h_t</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">i_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_t</span> <span class="o">+</span> <span class="n">i_t</span> <span class="o">*</span> <span class="n">h_</span><span class="p">(</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">h0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>
</code></pre>

<p>For each element in the input sequence, each layer computes the following
function:
where <code class="prettyprint">h_t</code> is the hidden state at time t, <code class="prettyprint">x_t</code> is the hidden
state of the previous layer at time t or input_t for the first layer,
and <code class="prettyprint">r_t</code>, <code class="prettyprint">i_t</code>, <code class="prettyprint">n_t</code> are the reset, input, and new gates, respectively.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>num_layers</td>
<td></td>
<td>the size of the convolving kernel.</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
<tr>
<td>batch_first</td>
<td></td>
<td>If True, then the input tensor is provided as (batch, seq, feature)</td>
</tr>
<tr>
<td>dropout</td>
<td></td>
<td>If non-zero, introduces a dropout layer on the outputs of each RNN layer</td>
</tr>
<tr>
<td>bidirectional</td>
<td>False</td>
<td>If True, becomes a bidirectional RNN.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (seq_len x batch x input_size) tensor containing the features of the input sequence.</td>
</tr>
<tr>
<td>h_0</td>
<td></td>
<td>A (num_layers x batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>output</td>
<td>A (seq_len x batch x hidden_size) tensor containing the output features (h_t) from the last layer of the RNN, for each t</td>
</tr>
<tr>
<td>h_n</td>
<td>A (num_layers x batch x hidden_size) tensor containing the hidden state for t=seq_len</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih_l[k]</td>
<td>the learnable input-hidden weights of the k-th layer (W_ir</td>
</tr>
<tr>
<td>weight_hh_l[k]</td>
<td>the learnable hidden-hidden weights of the k-th layer (W_hr</td>
</tr>
<tr>
<td>bias_ih_l[k]</td>
<td>the learnable input-hidden bias of the k-th layer (b_ir</td>
</tr>
<tr>
<td>bias_hh_l[k]</td>
<td>the learnable hidden-hidden bias of the k-th layer (W_hr</td>
</tr>
</tbody></table>

<h3 id="rnncell">RNNCell</h3>

<p>An Elman RNN cell with tanh or ReLU non-linearity.</p>
<pre class="highlight python"><code><span class="n">h</span><span class="s">' = tanh(w_ih * x + b_ih  +  w_hh * h + b_hh)</span><span class="err">
</span></code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">hx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hx</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">hx</span>
</code></pre>

<p>If nonlinearity=&#39;relu&rsquo;, then ReLU is used in place of tanh.</p>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
<tr>
<td>nonlinearity</td>
<td>&#39;tanh&rsquo;</td>
<td>The non-linearity to use [&#39;tanh&rsquo;</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (batch x input_size) tensor containing input features</td>
</tr>
<tr>
<td>hidden</td>
<td></td>
<td>A (batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>h&rsquo;</td>
<td>A (batch x hidden_size) tensor containing the next hidden state for each element in the batch</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih</td>
<td>the learnable input-hidden weights, of shape (input_size x hidden_size)</td>
</tr>
<tr>
<td>weight_hh</td>
<td>the learnable hidden-hidden weights, of shape (hidden_size x hidden_size)</td>
</tr>
<tr>
<td>bias_ih</td>
<td>the learnable input-hidden bias, of shape (hidden_size)</td>
</tr>
<tr>
<td>bias_hh</td>
<td>the learnable hidden-hidden bias, of shape (hidden_size)</td>
</tr>
</tbody></table>

<h3 id="lstmcell">LSTMCell</h3>

<p>A long short-term memory (LSTM) cell.</p>
<pre class="highlight python"><code><span class="n">i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ii</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_ii</span> <span class="o">+</span> <span class="n">W_hi</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hi</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_if</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_if</span> <span class="o">+</span> <span class="n">W_hf</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hf</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">W_ig</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_ig</span> <span class="o">+</span> <span class="n">W_hc</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hg</span><span class="p">)</span>
<span class="n">o</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_io</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_io</span> <span class="o">+</span> <span class="n">W_ho</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_ho</span><span class="p">)</span>
<span class="n">c</span><span class="s">' = f * c + i * c</span><span class="err">
</span><span class="s">h'</span> <span class="o">=</span> <span class="n">o</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c_t</span><span class="p">)</span>
</code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">cx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">hx</span><span class="p">,</span> <span class="n">cx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">hx</span><span class="p">,</span> <span class="n">cx</span><span class="p">))</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">hx</span>
</code></pre>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (batch x input_size) tensor containing input features</td>
</tr>
<tr>
<td>hidden</td>
<td></td>
<td>A (batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>h&rsquo;</td>
<td>A (batch x hidden_size) tensor containing the next hidden state for each element in the batch</td>
</tr>
<tr>
<td>c&rsquo;</td>
<td>A (batch x hidden_size) tensor containing the next cell state for each element in the batch</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih</td>
<td>the learnable input-hidden weights, of shape (input_size x hidden_size)</td>
</tr>
<tr>
<td>weight_hh</td>
<td>the learnable hidden-hidden weights, of shape (hidden_size x hidden_size)</td>
</tr>
<tr>
<td>bias_ih</td>
<td>the learnable input-hidden bias, of shape (hidden_size)</td>
</tr>
<tr>
<td>bias_hh</td>
<td>the learnable hidden-hidden bias, of shape (hidden_size)</td>
</tr>
</tbody></table>

<h3 id="grucell">GRUCell</h3>

<p>A gated recurrent unit (GRU) cell</p>
<pre class="highlight python"><code><span class="n">r</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ir</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_ir</span> <span class="o">+</span> <span class="n">W_hr</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hr</span><span class="p">)</span>
<span class="n">i</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">W_ii</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b_ii</span> <span class="o">+</span> <span class="n">W_hi</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b_hi</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">W_in</span> <span class="n">x</span> <span class="o">+</span> <span class="n">resetgate</span> <span class="o">*</span> <span class="n">W_hn</span> <span class="n">h</span><span class="p">)</span>
<span class="n">h</span><span class="s">' = (1 - i) * n + i * h</span><span class="err">
</span></code></pre>
<pre class="highlight python"><code><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">hx</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">hx</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">hx</span><span class="p">)</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">hx</span>
</code></pre>

<h4 id="constructor-arguments">Constructor Arguments</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input_size</td>
<td></td>
<td>The number of expected features in the input x</td>
</tr>
<tr>
<td>hidden_size</td>
<td></td>
<td>The number of features in the hidden state h</td>
</tr>
<tr>
<td>bias</td>
<td>True</td>
<td>If False, then the layer does not use bias weights b_ih and b_hh.</td>
</tr>
</tbody></table>

<h4 id="inputs">Inputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td></td>
<td>A (batch x input_size) tensor containing input features</td>
</tr>
<tr>
<td>hidden</td>
<td></td>
<td>A (batch x hidden_size) tensor containing the initial hidden state for each element in the batch.</td>
</tr>
</tbody></table>

<h4 id="outputs">Outputs</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>h&rsquo;</td>
<td>A (batch x hidden_size) tensor containing the next hidden state for each element in the batch</td>
</tr>
</tbody></table>

<h4 id="members">Members</h4>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>weight_ih</td>
<td>the learnable input-hidden weights, of shape (input_size x hidden_size)</td>
</tr>
<tr>
<td>weight_hh</td>
<td>the learnable hidden-hidden weights, of shape (hidden_size x hidden_size)</td>
</tr>
<tr>
<td>bias_ih</td>
<td>the learnable input-hidden bias, of shape (hidden_size)</td>
</tr>
<tr>
<td>bias_hh</td>
<td>the learnable hidden-hidden bias, of shape (hidden_size)</td>
</tr>
</tbody></table>
