<h1 id="torch-nn">torch.nn</h1>

<h2 id="logsoftmax">LogSoftmax</h2>

<p>Applies the Log(Softmax(x)) function to an n-dimensional input Tensor.</p>

<p>The LogSoftmax formulation can be simplified as
     f_i(x) = log(1 / a * exp(x_i)) where a = sum_j exp(x_j) .</p>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>[ : , :]  (2D Tensor, any size)</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="logsoftmax.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="relu">ReLU</h2>

<p>Applies the rectified linear unit function element-wise ReLU(x)= max(0,x)</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="relu.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="logsigmoid">LogSigmoid</h2>

<p>Applies element-wise LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))</p>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="logsigmoid.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="prelu">PReLU</h2>

<p>Applies element-wise the function PReLU(x) = max(0,x) + a * min(0,x)</p>

<p>Here &ldquo;a&rdquo; is a learnable parameter.
When called without arguments, nn.PReLU() uses a single parameter &ldquo;a&rdquo;
across all input channels. If called with nn.PReLU(nChannels), a separate
&ldquo;a&rdquo; is used for each input channel.
Note that weight decay should not be used when learning &ldquo;a&rdquo; for good
performance.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_parameters</td>
<td>1</td>
<td>number of &ldquo;a&rdquo; to learn.</td>
</tr>
<tr>
<td>init</td>
<td>0.25</td>
<td>the initial value of &ldquo;a&rdquo;.</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="prelu.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="softmax2d">Softmax2d</h2>

<p>Applies SoftMax over features to each spatial location</p>

<p>When given an image of Channels x Height x Width, it will
apply Softmax to each location [Channels, h_i, w_j]</p>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>[ : , : , : , : ] (4D Tensor, any size)</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax2d</span><span class="p">()</span>
<span class="c"># you softmax over the 2nd dimension</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="relu6">ReLU6</h2>

<p>Applies the element-wise function ReLU6(x) = min( max(0,x), 6)</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="relu6.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="tanh">Tanh</h2>

<p>Applies element-wise, Tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))</p>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="tanh.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="softplus">Softplus</h2>

<p>Applies element-wise SoftPlus(x) = 1/beta * log(1 + exp(beta * x_i))</p>

<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.
For numerical stability the implementation reverts to the linear function
for inputs above a certain value.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>beta</td>
<td>1</td>
<td>the beta value for the Softplus formulation.</td>
</tr>
<tr>
<td>threshold</td>
<td>20</td>
<td>values above this revert to a linear function.</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="softplus.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="threshold">Threshold</h2>

<p>Thresholds each element of the input Tensor</p>

<p>Threshold is defined as:
     y =  x        if x &gt;= threshold
          value    if x &lt;  threshold</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>threshold</td>
<td></td>
<td>The value to threshold at</td>
</tr>
<tr>
<td>value</td>
<td></td>
<td>The value to replace with</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>Tensor of same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape
Examples:</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="softmin">Softmin</h2>

<p>Applies the Softmin function to an n-dimensional input Tensor</p>

<p>rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1
Softmin(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)
             where shift = max_i - x_i</p>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>[ : , :]  (2D Tensor, any size)</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="softmin.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="softshrink">Softshrink</h2>

<p>Applies the soft shrinkage function elementwise</p>

<p>SoftShrinkage operator is defined as:
    f(x) = x-lambda, if x &gt; lambda &gt;  f(x) = x+lambda, if x &lt; -lambda
    f(x) = 0, otherwise</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>lambd</td>
<td>0.5</td>
<td>the lambda value for the Softshrink formulation.</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="sshrink.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="elu">ELU</h2>

<p>Applies element-wise, ELU(x) = max(0,x) + min(0, alpha * (exp(x) - 1))</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>alpha</td>
<td>1.0</td>
<td>the alpha value for the ELU formulation.</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="elu.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="hardshrink">Hardshrink</h2>

<p>Applies the hard shrinkage function element-wise</p>

<p>Hardshrink is defined as f(x) = x, if x &gt;  lambda
                         f(x) = x, if x &lt; -lambda
                         f(x) = 0, otherwise</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>lambd</td>
<td>0.5</td>
<td>the lambda value for the Hardshrink formulation.</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="hshrink.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="hardtanh">Hardtanh</h2>

<p>Applies the HardTanh function element-wise</p>

<p>HardTanh is defined as:
   f(x) = +1, if x  &gt;  1
   f(x) = -1, if x  &lt; -1
   f(x) =  x,  otherwise
The range of the linear region [-1, 1] can be adjusted</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>min_value</td>
<td></td>
<td>minimum value of the linear region range</td>
</tr>
<tr>
<td>max_value</td>
<td></td>
<td>maximum value of the linear region range</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="htanh.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HardTanh</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="softsign">Softsign</h2>

<p>Applies element-wise, the function Softsign(x) = x / (1 + |x|)</p>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="softsign.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="leakyrelu">LeakyReLU</h2>

<p>Applies element-wise, f(x) = max(0, x) + negative_slope * min(0, x)</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>negative_slope</td>
<td>1e-2</td>
<td>Controls the angle of the negative slope.</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="sigmoid">Sigmoid</h2>

<p>Applies the element-wise function sigmoid(x) = 1 / ( 1 + exp(-x))</p>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="sigmoid.png" ></p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="tanhshrink">Tanhshrink</h2>

<p>Applies element-wise, Tanhshrink(x) = x - Tanh(x)</p>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>Tensor of any size and dimension</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanhshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h2 id="softmax">Softmax</h2>

<p>Applies the Softmax function to an n-dimensional input Tensor</p>

<p>rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1</p>

<p>Softmax is defined as f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)
                      where shift = max_i x_i</p>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h3 id="input-shape">Input Shape</h3>

<p>[: , :] (2D Tensor, any size)</p>

<h3 id="output-shape">Output Shape</h3>

<p>Same as input shape</p>

<p><img src="softmax.png" >
Notes:
    Note that this module doesn&rsquo;t work directly with NLLLoss,
    which expects the Log to be computed between the Softmax and itself.
    Use Logsoftmax instead (it&rsquo;s faster).</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>
