<h1 id="torch-nn">torch.nn</h1>

<h2 id="batchnorm2d">BatchNorm2d</h2>

<p>Applies Batch Normalization over a 4d input that is seen as a mini-batch of 3d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1 
During evaluation, this running mean/variance is used for normalization.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>num_features from an expected input of size batch_size x num_features x height x width</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features , *, * ]</td>
<td>4D Tensor of batch_size x num_features x height x width</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a normalized tensor in the batch dimension</p>

<h2 id="container">Container</h2>

<p>This is the base container class for all neural networks you would define.</p>
<pre class="highlight python"><code><span class="c"># Example of using Container</span>
 <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Container</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
            <span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
            <span class="n">relu</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
         <span class="p">)</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">output</span>
 <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># one can add modules to the container after construction</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s">'pool1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre>

<p>You will subclass your container from this class.
In the constructor you define the modules that you would want to use, 
and in the <strong>call</strong> function you use the constructed modules in 
your operations.</p>

<p>To make it easier to understand, given is a small example.</p>

<p>One can also add new modules to a container after construction.
You can do this with the add_module function.</p>

<p>The container has one additional method <code class="prettyprint">parameters()</code> which
returns the list of learnable parameters in the container instance.</p>

<h2 id="logsoftmax">LogSoftmax</h2>

<p>Applies the Log(Softmax(x)) function to an n-dimensional input Tensor.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>The LogSoftmax formulation can be simplified as
     f_i(x) = log(1 / a * exp(x_i)) where a = sum_j exp(x_j) .</p>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * ]</td>
<td>2D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input with
    values in the range [-inf, 0)</p>

<p><img src="image/logsoftmax.png" ></p>

<h2 id="relu">ReLU</h2>

<p>Applies the rectified linear unit function element-wise ReLU(x)= max(0,x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/relu.png" ></p>

<h2 id="logsigmoid">LogSigmoid</h2>

<p>Applies element-wise LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/logsigmoid.png" ></p>

<h2 id="prelu">PReLU</h2>

<p>Applies element-wise the function PReLU(x) = max(0,x) + a * min(0,x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>Here &ldquo;a&rdquo; is a learnable parameter.
When called without arguments, nn.PReLU() uses a single parameter &ldquo;a&rdquo;
across all input channels. If called with nn.PReLU(nChannels), a separate
&ldquo;a&rdquo; is used for each input channel.
Note that weight decay should not be used when learning &ldquo;a&rdquo; for good
performance.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_parameters</td>
<td>1</td>
<td>number of &ldquo;a&rdquo; to learn.</td>
</tr>
<tr>
<td>init</td>
<td>0.25</td>
<td>the initial value of &ldquo;a&rdquo;.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/prelu.png" ></p>

<h2 id="softmax2d">Softmax2d</h2>

<p>Applies SoftMax over features to each spatial location</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax2d</span><span class="p">()</span>
<span class="c"># you softmax over the 2nd dimension</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>When given an image of Channels x Height x Width, it will
apply Softmax to each location [Channels, h_i, w_j]</p>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , * , * ]</td>
<td>4D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input with
    values in the range [0, 1]</p>

<h2 id="relu6">ReLU6</h2>

<p>Applies the element-wise function ReLU6(x) = min( max(0,x), 6)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/relu6.png" ></p>

<h2 id="batchnorm3d">BatchNorm3d</h2>

<p>Applies Batch Normalization over a 5d input that is seen as a mini-batch of 4d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1 
During evaluation, this running mean/variance is used for normalization.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>num_features from an expected input of size batch_size x num_features x height x width</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features , * , * , * ]</td>
<td>5D Tensor of batch_size x num_features x depth x height x width</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a normalized tensor in the batch dimension</p>

<h2 id="tanh">Tanh</h2>

<p>Applies element-wise, Tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/tanh.png" ></p>

<h2 id="softplus">Softplus</h2>

<p>Applies element-wise SoftPlus(x) = 1/beta * log(1 + exp(beta * x_i))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.
For numerical stability the implementation reverts to the linear function
for inputs above a certain value.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>beta</td>
<td>1</td>
<td>the beta value for the Softplus formulation.</td>
</tr>
<tr>
<td>threshold</td>
<td>20</td>
<td>values above this revert to a linear function.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/softplus.png" ></p>

<h2 id="threshold">Threshold</h2>

<p>Thresholds each element of the input Tensor</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>Threshold is defined as:
     y =  x        if x &gt;= threshold
          value    if x &lt;  threshold</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>threshold</td>
<td></td>
<td>The value to threshold at</td>
</tr>
<tr>
<td>value</td>
<td></td>
<td>The value to replace with</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>Tensor of same dimension and shape as the input</p>

<h2 id="softmin">Softmin</h2>

<p>Applies the Softmin function to an n-dimensional input Tensor</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1
Softmin(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)
             where shift = max_i - x_i</p>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * ]</td>
<td>2D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input, with
    values in the range [0, 1]</p>

<p><img src="image/softmin.png" ></p>

<h2 id="softshrink">Softshrink</h2>

<p>Applies the soft shrinkage function elementwise</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>SoftShrinkage operator is defined as:
    f(x) = x-lambda, if x &gt; lambda &gt;  f(x) = x+lambda, if x &lt; -lambda
    f(x) = 0, otherwise</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>lambd</td>
<td>0.5</td>
<td>the lambda value for the Softshrink formulation.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/sshrink.png" ></p>

<h2 id="batchnorm1d">BatchNorm1d</h2>

<p>Applies Batch Normalization over a 2d input that is seen as a mini-batch of 1d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1 
During evaluation, this running mean/variance is used for normalization.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>the size of each 1D input in the mini-batch</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features ]</td>
<td>2D Tensor of nBatches x num_features</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a normalized tensor in the batch dimension</p>

<h2 id="elu">ELU</h2>

<p>Applies element-wise, ELU(x) = max(0,x) + min(0, alpha * (exp(x) - 1))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>alpha</td>
<td>1.0</td>
<td>the alpha value for the ELU formulation.</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/elu.png" ></p>

<h2 id="hardshrink">Hardshrink</h2>

<p>Applies the hard shrinkage function element-wise</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>Hardshrink is defined as f(x) = x, if x &gt;  lambda
                         f(x) = x, if x &lt; -lambda
                         f(x) = 0, otherwise</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>lambd</td>
<td>0.5</td>
<td>the lambda value for the Hardshrink formulation.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/hshrink.png" ></p>

<h2 id="hardtanh">Hardtanh</h2>

<p>Applies the HardTanh function element-wise</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HardTanh</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>HardTanh is defined as:
   f(x) = +1, if x  &gt;  1
   f(x) = -1, if x  &lt; -1
   f(x) =  x,  otherwise
The range of the linear region [-1, 1] can be adjusted</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>min_value</td>
<td></td>
<td>minimum value of the linear region range</td>
</tr>
<tr>
<td>max_value</td>
<td></td>
<td>maximum value of the linear region range</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/htanh.png" ></p>

<h2 id="softsign">Softsign</h2>

<p>Applies element-wise, the function Softsign(x) = x / (1 + |x|)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/softsign.png" ></p>

<h2 id="leakyrelu">LeakyReLU</h2>

<p>Applies element-wise, f(x) = max(0, x) + negative_slope * min(0, x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>negative_slope</td>
<td>1e-2</td>
<td>Controls the angle of the negative slope.</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h2 id="sigmoid">Sigmoid</h2>

<p>Applies the element-wise function sigmoid(x) = 1 / ( 1 + exp(-x))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/sigmoid.png" ></p>

<h2 id="tanhshrink">Tanhshrink</h2>

<p>Applies element-wise, Tanhshrink(x) = x - Tanh(x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanhshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h2 id="softmax">Softmax</h2>

<p>Applies the Softmax function to an n-dimensional input Tensor</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1</p>

<p>Softmax is defined as f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)
                      where shift = max_i x_i</p>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * ]</td>
<td>2D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input with
    values in the range [0, 1]</p>

<p><img src="image/softmax.png" >
Notes:
    Note that this module doesn&rsquo;t work directly with NLLLoss,
    which expects the Log to be computed between the Softmax and itself.
    Use Logsoftmax instead (it&rsquo;s faster).</p>
