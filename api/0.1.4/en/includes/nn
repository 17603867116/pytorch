<h1 id="torch-nn">torch.nn</h1>

<h2 id="avgpool2d">AvgPool2d</h2>

<p>Applies a 2D average pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">H</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">oH</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">h_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span> <span class="o">*</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">KH</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kW</span><span class="p">}</span>  <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">stride_h</span> <span class="o">*</span> <span class="n">h_i</span> <span class="o">+</span> <span class="n">kh</span><span class="p">)][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">w_i</span> <span class="o">+</span> <span class="n">kw</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. </p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding to be added. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape = minibatch x channels x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h2 id="avgpool3d">AvgPool3d</h2>

<p>Applies a 3D average pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. </p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a average over. Can be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (st x sh x sw).</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, *, * ]</td>
<td>Input is minibatch x channels x iT x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, *, * ]</td>
<td>Output shape = minibatch x channels x floor((iT  + 2*padT - kT) / sT + 1) x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h2 id="batchnorm1d">BatchNorm1d</h2>

<p>Applies Batch Normalization over a 2d input that is seen as a mini-batch of 1d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1 
During evaluation, this running mean/variance is used for normalization.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>the size of each 1D input in the mini-batch</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features ]</td>
<td>2D Tensor of nBatches x num_features</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a normalized tensor in the batch dimension</p>

<h2 id="batchnorm2d">BatchNorm2d</h2>

<p>Applies Batch Normalization over a 4d input that is seen as a mini-batch of 3d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1 
During evaluation, this running mean/variance is used for normalization.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>num_features from an expected input of size batch_size x num_features x height x width</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features , *, * ]</td>
<td>4D Tensor of batch_size x num_features x height x width</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a normalized tensor in the batch dimension</p>

<h2 id="batchnorm3d">BatchNorm3d</h2>

<p>Applies Batch Normalization over a 5d input that is seen as a mini-batch of 4d inputs</p>
<pre class="highlight python"><code>              <span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span>  <span class="o">-----------------------------</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">beta</span>
      <span class="n">standard_deviation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="c"># Without Learnable Parameters</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and gamma and beta are learnable parameter vectors
of size N (where N is the input size).</p>

<p>During training, this layer keeps a running estimate of its computed mean
and variance. The running sum is kept with a default momentum of 0.1 
During evaluation, this running mean/variance is used for normalization.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_features</td>
<td></td>
<td>num_features from an expected input of size batch_size x num_features x height x width</td>
</tr>
<tr>
<td>eps</td>
<td>1e-5</td>
<td>a value added to the denominator for numerical stability.</td>
</tr>
<tr>
<td>momentum</td>
<td>0.1</td>
<td>the value used for the running_mean and running_var computation.</td>
</tr>
<tr>
<td>affine</td>
<td></td>
<td>a boolean value that when set to true, gives the layer learnable affine parameters.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , num_features , * , * , * ]</td>
<td>5D Tensor of batch_size x num_features x depth x height x width</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a normalized tensor in the batch dimension</p>

<h2 id="container">Container</h2>

<p>This is the base container class for all neural networks you would define.</p>
<pre class="highlight python"><code><span class="c"># Example of using Container</span>
 <span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Container</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
            <span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
            <span class="n">relu</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
         <span class="p">)</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">output</span>
 <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># one can add modules to the container after construction</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s">'pool1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre>

<p>You will subclass your container from this class.
In the constructor you define the modules that you would want to use, 
and in the <strong>call</strong> function you use the constructed modules in 
your operations.</p>

<p>To make it easier to understand, given is a small example.</p>

<p>One can also add new modules to a container after construction.
You can do this with the add_module function.</p>

<p>The container has one additional method <code class="prettyprint">parameters()</code> which
returns the list of learnable parameters in the container instance.</p>

<h2 id="conv1d">Conv1d</h2>

<p>Applies a 1D convolution over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">iC</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">oC</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">oc_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span><span class="p">[</span><span class="n">oc_i</span><span class="p">]</span> 
            <span class="o">+</span> <span class="n">sum_iC</span> <span class="n">sum_</span><span class="p">{</span><span class="n">ow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">oW</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kw</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">kW</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> 
                <span class="n">weight</span><span class="p">[</span><span class="n">oc_i</span><span class="p">][</span><span class="n">ic_i</span><span class="p">][</span><span class="n">kw</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">ic_i</span><span class="p">][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">ow</span> <span class="o">+</span> <span class="n">kw</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. </p>

<p>Note that depending of the size of your kernel, several (of the last)
columns of the input might be lost. It is up to the user
to add proper padding.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel.</td>
</tr>
<tr>
<td>stride</td>
<td></td>
<td>the stride of the convolving kernel.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * ]</td>
<td>Input is minibatch x in_channels x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * ]</td>
<td>Output shape is precisely minibatch x out_channels x floor((iW  + 2*padW - kW) / dW + 1)</td>
</tr>
</tbody></table>

<p>Members:
    weight: the learnable weights of the module of shape (out_channels x in_channels x kW)
    bias:   the learnable bias of the module of shape (out_channels)</p>

<h2 id="conv2d">Conv2d</h2>

<p>Applies a 2D convolution over an input image composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">iC</span> <span class="n">x</span> <span class="n">H</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">oC</span> <span class="n">x</span> <span class="n">oH</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">oc_i</span><span class="p">][</span><span class="n">h_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span><span class="p">[</span><span class="n">oc_i</span><span class="p">]</span> 
            <span class="o">+</span> <span class="n">sum_iC</span> <span class="n">sum_</span><span class="p">{</span><span class="n">oh</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">oH</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">ow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">oW</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kh</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">kH</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> <span class="n">sum_</span><span class="p">{</span><span class="n">kw</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">kW</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span> 
                <span class="n">weight</span><span class="p">[</span><span class="n">oc_i</span><span class="p">][</span><span class="n">ic_i</span><span class="p">][</span><span class="n">kh</span><span class="p">][</span><span class="n">kw</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">ic_i</span><span class="p">][</span><span class="n">stride_h</span> <span class="o">*</span> <span class="n">oh</span> <span class="o">+</span> <span class="n">kh</span><span class="p">)][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">ow</span> <span class="o">+</span> <span class="n">kw</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># With square kernels and equal stride</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># non-square kernels and unequal stride and with padding</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="c"># non-square kernels and unequal stride and with padding and dilation</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. </p>

<p>Note that depending of the size of your kernel, several (of the last)
columns or rows of the input image might be lost. It is up to the user
to add proper padding in images.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>1</td>
<td>the stride of the convolving kernel. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit zero padding on the input. Can be a single number s or a tuple.</td>
</tr>
<tr>
<td>dilation</td>
<td>None</td>
<td>If given, will do dilated (or atrous) convolutions. Can be a single number s or a tuple.</td>
</tr>
<tr>
<td>no_bias</td>
<td>False</td>
<td>If set to true, the layer will not learn an additive bias.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * , * ]</td>
<td>Input is minibatch x in_channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * , * ]</td>
<td>Output shape is precisely minibatch x out_channels x floor((iH  + 2*padH - kH) / dH + 1) x floor((iW  + 2*padW - kW) / dW + 1)</td>
</tr>
</tbody></table>

<p>Members:
    weight: the learnable weights of the module of shape (out_channels x in_channels x kH x kW)
    bias:   the learnable bias of the module of shape (out_channels)</p>

<h2 id="conv3d">Conv3d</h2>

<p>Applies a 3D convolution over an input image composed of several input</p>
<pre class="highlight python"><code><span class="c"># With square kernels and equal stride</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># non-square kernels and unequal stride and with padding</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. </p>

<p>Note that depending of the size of your kernel, several (of the last)
columns or rows of the input image might be lost. It is up to the user
to add proper padding in images.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel. Can be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>1</td>
<td>the stride of the convolving kernel. Can be a single number s or a tuple (kt x sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit zero padding on the input. Can be a single number s or a tuple.</td>
</tr>
<tr>
<td>no_bias</td>
<td>False</td>
<td>If set to true, the layer will not learn an additive bias.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * , * , * ]</td>
<td>Input is minibatch x in_channels x iT x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * , * , * ]</td>
<td>Output shape is precisely minibatch x out_channels x floor((iT  + 2*padT - kT) / dT + 1) x floor((iH  + 2*padH - kH) / dH + 1) x floor((iW  + 2*padW - kW) / dW + 1)</td>
</tr>
</tbody></table>

<p>Members:
    weight: the learnable weights of the module of shape (out_channels x in_channels x kT x kH x kW)
    bias:   the learnable bias of the module of shape (out_channels)</p>

<h2 id="dropout">Dropout</h2>

<p>Randomly zeroes some of the elements of the input tensor.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The elements to zero are randomized on every forward call.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>p</td>
<td>0.5</td>
<td>probability of an element to be zeroed.</td>
</tr>
<tr>
<td>inplace</td>
<td>false</td>
<td>If set to True, will do this operation in-place.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Input can be of any shape</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output is of the same shape as input</td>
</tr>
</tbody></table>

<h2 id="dropout2d">Dropout2d</h2>

<p>Randomly zeroes whole channels of the input tensor.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The input is 4D (batch x channels, height, width) and each channel 
is of size (1, height, width).
The channels to zero are randomized on every forward call.
Usually the input comes from Conv2d modules.</p>

<p>As described in the paper &ldquo;Efficient Object Localization Using Convolutional
Networks&rdquo; (http:arxiv.org/abs/1411.4280), if adjacent pixels within
feature maps are strongly correlated (as is normally the case in early
convolution layers) then iid dropout will not regularize the activations
and will otherwise just result in an effective learning rate decrease.
In this case, nn.Dropout2d will help promote independence between
feature maps and should be used instead.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>p</td>
<td>0.5</td>
<td>probability of an element to be zeroed.</td>
</tr>
<tr>
<td>inplace</td>
<td>false</td>
<td>If set to True, will do this operation in-place.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[*, *, *, *]</td>
<td>Input can be of any sizes of 4D shape</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output is of the same shape as input</td>
</tr>
</tbody></table>

<h2 id="dropout3d">Dropout3d</h2>

<p>Randomly zeroes whole channels of the input tensor.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout3d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>The input is 5D (batch x channels, depth, height, width) and each channel 
is of size (1, depth, height, width).
The channels to zero are randomized on every forward call.
Usually the input comes from Conv3d modules.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>p</td>
<td>0.5</td>
<td>probability of an element to be zeroed.</td>
</tr>
<tr>
<td>inplace</td>
<td>false</td>
<td>If set to True, will do this operation in-place.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[*, *, *, *, *]</td>
<td>Input can be of any sizes of 5D shape</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output is of the same shape as input</td>
</tr>
</tbody></table>

<h2 id="elu">ELU</h2>

<p>Applies element-wise, ELU(x) = max(0,x) + min(0, alpha * (exp(x) - 1))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>alpha</td>
<td>1.0</td>
<td>the alpha value for the ELU formulation.</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/elu.png" ></p>

<h2 id="embedding">Embedding</h2>

<p>A simple lookup table that stores embeddings of a fixed dictionary and size</p>
<pre class="highlight python"><code><span class="c"># an Embedding module containing 10 tensors of size 3</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="c"># a batch of 2 samples of 4 indices each</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>This module is often used to store word embeddings and retrieve them using indices.
The input to the module is a list of indices, and the output is the corresponding
word embeddings.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_embeddings</td>
<td></td>
<td>size of the dictionary of embeddings</td>
</tr>
<tr>
<td>embedding_dim</td>
<td></td>
<td>the size of each embedding vector</td>
</tr>
<tr>
<td>padding_idx</td>
<td>-1</td>
<td>If given, pads the output with zeros whenever it encounters the index.</td>
</tr>
<tr>
<td>max_norm</td>
<td>None</td>
<td>If given, will renormalize the embeddings to always have a norm lesser than this</td>
</tr>
<tr>
<td>norm_type</td>
<td></td>
<td>The p of the p-norm to compute for the max_norm option</td>
</tr>
<tr>
<td>scale_grad_by_freq</td>
<td></td>
<td>if given, this will scale gradients by the frequency of the words in the dictionary.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ *, * ]</td>
<td>Input is a 2D mini_batch LongTensor of m x n indices to extract from the Embedding dictionary</td>
</tr>
<tr>
<td>output</td>
<td>[ * , *, * ]</td>
<td>Output shape = m x n x embedding_dim</td>
</tr>
</tbody></table>

<h2 id="fractionalmaxpool2d">FractionalMaxPool2d</h2>

<p>Applies a 2D fractional max pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, and target output size 13x12</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="c"># pool of square window and target output size being half of input image size</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FractionalMaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. </p>

<p>Fractiona MaxPooling is described in detail in the paper <a href="http://arxiv.org/abs/1412.6071">&ldquo;Fractional Max-Pooling&rdquo; by Ben Graham</a>
The max-pooling operation is applied in kHxkW regions by a stochastic
step size determined by the target output size.
The number of output features is equal to the number of input planes.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>output_size</td>
<td></td>
<td>the target output size of the image of the form oH x oW. Can be a tuple (oH, oW) or a single number oH for a square image oH x oH</td>
</tr>
<tr>
<td>output_ratio</td>
<td></td>
<td>If one wants to have an output size as a ratio of the input size, this option can be given. This has to be a number or tuple in the range (0, 1)</td>
</tr>
<tr>
<td>return_indices</td>
<td>False</td>
<td>if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d .</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape = minibatch x channels x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h2 id="fullconv2d">FullConv2d</h2>

<p>Applies a 2D deconvolution operator over an input image composed of several input</p>
<pre class="highlight python"><code><span class="c"># With square kernels and equal stride</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FullConv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># non-square kernels and unequal stride and with padding</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. 
The deconvolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.
This module can be seen as the exact reverse of the Conv2d module.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>1</td>
<td>the stride of the convolving kernel. Can be a single number or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit zero padding on the input. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>output_padding</td>
<td>0</td>
<td>A padding of 0 or 1 pixels that should be added to the output. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>no_bias</td>
<td>False</td>
<td>If set to true, the layer will not learn an additive bias.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * , * ]</td>
<td>Input is minibatch x in_channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * , * ]</td>
<td>Output shape is precisely minibatch x out_channels x (iH - 1) * sH - 2*padH + kH + output_paddingH x (iW - 1) * sW - 2*padW + kW</td>
</tr>
</tbody></table>

<p>Members:
    weight: the learnable weights of the module of shape (in_channels x out_channels x kH x kW)
    bias:   the learnable bias of the module of shape (out_channels)</p>

<h2 id="fullconv3d">FullConv3d</h2>

<p>Applies a 3D deconvolution operator over an input image composed of several input</p>
<pre class="highlight python"><code><span class="c"># With square kernels and equal stride</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FullConv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># non-square kernels and unequal stride and with padding</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. 
The deconvolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.
This module can be seen as the exact reverse of the Conv3d module.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_channels</td>
<td></td>
<td>The number of expected input channels in the image given as input</td>
</tr>
<tr>
<td>out_channels</td>
<td></td>
<td>The number of output channels the convolution layer will produce</td>
</tr>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the convolving kernel. Can be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>1</td>
<td>the stride of the convolving kernel. Can be a single number or a tuple (st x sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit zero padding on the input. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>output_padding</td>
<td>0</td>
<td>A padding of 0 or 1 pixels that should be added to the output. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>no_bias</td>
<td>False</td>
<td>If set to true, the layer will not learn an additive bias.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , in_channels  , * , * , * ]</td>
<td>Input is minibatch x in_channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , out_channels , * , * , * ]</td>
<td>Output shape is precisely minibatch x out_channels x (iT - 1) * sT - 2*padT + kT + output_paddingT x (iH - 1) * sH - 2*padH + kH + output_paddingH x (iW - 1) * sW - 2*padW + kW</td>
</tr>
</tbody></table>

<p>Members:
    weight: the learnable weights of the module of shape (in_channels x out_channels x kT x kH x kW)
    bias:   the learnable bias of the module of shape (out_channels)</p>

<h2 id="hardshrink">Hardshrink</h2>

<p>Applies the hard shrinkage function element-wise</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Hardshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>Hardshrink is defined as f(x) = x, if x &gt;  lambda
                         f(x) = x, if x &lt; -lambda
                         f(x) = 0, otherwise</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>lambd</td>
<td>0.5</td>
<td>the lambda value for the Hardshrink formulation.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/hshrink.png" ></p>

<h2 id="hardtanh">Hardtanh</h2>

<p>Applies the HardTanh function element-wise</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HardTanh</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>HardTanh is defined as:
   f(x) = +1, if x  &gt;  1
   f(x) = -1, if x  &lt; -1
   f(x) =  x,  otherwise
The range of the linear region [-1, 1] can be adjusted</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>min_value</td>
<td></td>
<td>minimum value of the linear region range</td>
</tr>
<tr>
<td>max_value</td>
<td></td>
<td>maximum value of the linear region range</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/htanh.png" ></p>

<h2 id="lppool2d">LPPool2d</h2>

<p>Applies a 2D power-average pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="c"># power-2 pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window of power 1.2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LPPool2d</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. 
On each window, the function computed is: f(X) = pow(sum(pow(X, p)), 1/p)
At p = infinity, one gets Max Pooling
At p = 1, one gets Average Pooling</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape = minibatch x channels x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h2 id="leakyrelu">LeakyReLU</h2>

<p>Applies element-wise, f(x) = max(0, x) + negative_slope * min(0, x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>negative_slope</td>
<td>1e-2</td>
<td>Controls the angle of the negative slope.</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h2 id="linear">Linear</h2>

<p>Applies a linear transformation to the incoming data, y = Ax + b</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre>

<p>The input is a 2D mini-batch of samples, each of size in_features
The output will be a 2D Tensor of size mini-batch x out_features</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>in_features</td>
<td></td>
<td>size of each input sample</td>
</tr>
<tr>
<td>out_features</td>
<td></td>
<td>size of each output sample</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[*, in_features]</td>
<td>Input can be of shape minibatch x in_features</td>
</tr>
<tr>
<td>output</td>
<td>[*, out_features]</td>
<td>Output is of shape minibatch x out_features</td>
</tr>
</tbody></table>

<p>Members:
    weight: the learnable weights of the module of shape (out_features x in_features)
    bias:   the learnable bias of the module of shape (out_features)</p>

<h2 id="logsigmoid">LogSigmoid</h2>

<p>Applies element-wise LogSigmoid(x) = log( 1 / (1 + exp(-x_i)))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/logsigmoid.png" ></p>

<h2 id="logsoftmax">LogSoftmax</h2>

<p>Applies the Log(Softmax(x)) function to an n-dimensional input Tensor.</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>The LogSoftmax formulation can be simplified as
     f_i(x) = log(1 / a * exp(x_i)) where a = sum_j exp(x_j) .</p>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * ]</td>
<td>2D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input with
    values in the range [-inf, 0)</p>

<p><img src="image/logsoftmax.png" ></p>

<h2 id="maxpool1d">MaxPool1d</h2>

<p>Applies a 1D max pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_</span><span class="p">{</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">K</span><span class="p">}</span> <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">w_i</span> <span class="o">+</span> <span class="n">k</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># pool of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. </p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a max over</td>
</tr>
<tr>
<td>stride</td>
<td></td>
<td>the stride of the window</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding to be added.</td>
</tr>
<tr>
<td>dilation</td>
<td>kernel_size</td>
<td>a parameter that controls the stride of elements in the window.</td>
</tr>
<tr>
<td>return_indices</td>
<td>False</td>
<td>if True, will return the indices along with the outputs. Useful when Unpooling later.</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , * ]</td>
<td>Input is minibatch x channels x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , * ]</td>
<td>Output shape = minibatch x channels x floor((iW  + 2*padW - kernel_size) / stride + 1)</td>
</tr>
</tbody></table>

<h2 id="maxpool2d">MaxPool2d</h2>

<p>Applies a 2D max pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="n">The</span> <span class="n">output</span> <span class="n">value</span> <span class="n">of</span> <span class="n">the</span> <span class="n">layer</span> <span class="k">with</span> <span class="nb">input</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">H</span> <span class="n">x</span> <span class="n">W</span><span class="p">)</span> <span class="ow">and</span> <span class="n">output</span> <span class="p">(</span><span class="n">b</span> <span class="n">x</span> <span class="n">C</span> <span class="n">x</span> <span class="n">oH</span> <span class="n">x</span> <span class="n">oW</span><span class="p">)</span>
<span class="n">can</span> <span class="n">be</span> <span class="n">precisely</span> <span class="n">described</span> <span class="k">as</span><span class="p">:</span>
<span class="n">output</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">h_i</span><span class="p">][</span><span class="n">w_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_</span><span class="p">{{</span><span class="n">kh</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">KH</span><span class="p">},</span> <span class="p">{</span><span class="n">kw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kW</span><span class="p">}}</span> <span class="nb">input</span><span class="p">[</span><span class="n">b_i</span><span class="p">][</span><span class="n">c_i</span><span class="p">][</span><span class="n">stride_h</span> <span class="o">*</span> <span class="n">h_i</span> <span class="o">+</span> <span class="n">kH</span><span class="p">)][</span><span class="n">stride_w</span> <span class="o">*</span> <span class="n">w_i</span> <span class="o">+</span> <span class="n">kW</span><span class="p">)]</span>
</code></pre>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. </p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding to be added. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>dilation</td>
<td>1</td>
<td>a parameter that controls the stride of elements in the window. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>return_indices</td>
<td>False</td>
<td>if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d .</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape = minibatch x channels x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h2 id="maxpool3d">MaxPool3d</h2>

<p>Applies a 3D max pooling over an input signal composed of several input</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c"># pool of non-square window</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span><span class="mi">44</span><span class="p">,</span> <span class="mi">31</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre>

<p>planes. </p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the window to take a max over. Can be a single number k (for a square kernel of k x k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (st x sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding to be added. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>dilation</td>
<td>1</td>
<td>a parameter that controls the stride of elements in the window. Can be a single number or a tuple.</td>
</tr>
<tr>
<td>return_indices</td>
<td>False</td>
<td>if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d .</td>
</tr>
<tr>
<td>ceil_mode</td>
<td></td>
<td>when True, will use &ldquo;ceil&rdquo; instead of &ldquo;floor&rdquo; to compute the output shape</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, *, * ]</td>
<td>Input is minibatch x channels x iT x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, *, * ]</td>
<td>Output shape = minibatch x channels x floor((iT  + 2*padT - kT) / sT + 1) x floor((iH  + 2*padH - kH) / sH + 1) x floor((iW  + 2*padW - kW) / sW + 1)</td>
</tr>
</tbody></table>

<h2 id="maxunpool2d">MaxUnpool2d</h2>

<p>Computes the inverse operation of MaxPool2d</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">unpooled_output</span> <span class="o">=</span> <span class="n">m2</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</code></pre>

<p>MaxPool2d is not invertible, as the locations of the max locations are lost.
MaxUnpool2d takes in as input the output of MaxPool2d and the indices of the Max locations
and computes the inverse.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the max window. Can be a single number k (for a square kernel of k x k) or a tuple (kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding that was added to the input. Can be a single number or a tuple.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, * ]</td>
<td>Input is minibatch x channels x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, * ]</td>
<td>Output shape = minibatch x channels x padH x (iH - 1) * sH + kH x padW x (iW - 1) * sW + kW</td>
</tr>
</tbody></table>

<h2 id="maxunpool3d">MaxUnpool3d</h2>

<p>Computes the inverse operation of MaxPool3d</p>
<pre class="highlight python"><code><span class="c"># pool of square window of size=3, stride=2</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_indices</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxUnpool3d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">input</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">unpooled_output</span> <span class="o">=</span> <span class="n">m2</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</code></pre>

<p>MaxPool3d is not invertible, as the locations of the max locations are lost.
MaxUnpool3d takes in as input the output of MaxPool3d and the indices of the Max locations
and computes the inverse.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>kernel_size</td>
<td></td>
<td>the size of the max window. Can be a single number k (for a square kernel of k x k) or a tuple (kt x kh x kw)</td>
</tr>
<tr>
<td>stride</td>
<td>kernel_size</td>
<td>the stride of the window. Can be a single number s or a tuple (st x sh x sw).</td>
</tr>
<tr>
<td>padding</td>
<td>0</td>
<td>implicit padding that was added to the input. Can be a single number or a tuple.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , *, *, * ]</td>
<td>Input is minibatch x channels x iT x iH x iW</td>
</tr>
<tr>
<td>output</td>
<td>[ * , * , *, *, * ]</td>
<td>Output shape = minibatch x channels x padT x (iT - 1) * sT + kT x padH x (iH - 1) * sH + kH x padW x (iW - 1) * sW + kW</td>
</tr>
</tbody></table>

<h2 id="prelu">PReLU</h2>

<p>Applies element-wise the function PReLU(x) = max(0,x) + a * min(0,x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>Here &ldquo;a&rdquo; is a learnable parameter.
When called without arguments, nn.PReLU() uses a single parameter &ldquo;a&rdquo;
across all input channels. If called with nn.PReLU(nChannels), a separate
&ldquo;a&rdquo; is used for each input channel.
Note that weight decay should not be used when learning &ldquo;a&rdquo; for good
performance.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>num_parameters</td>
<td>1</td>
<td>number of &ldquo;a&rdquo; to learn.</td>
</tr>
<tr>
<td>init</td>
<td>0.25</td>
<td>the initial value of &ldquo;a&rdquo;.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/prelu.png" ></p>

<h2 id="relu">ReLU</h2>

<p>Applies the rectified linear unit function element-wise ReLU(x)= max(0,x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/relu.png" ></p>

<h2 id="relu6">ReLU6</h2>

<p>Applies the element-wise function ReLU6(x) = min( max(0,x), 6)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/relu6.png" ></p>

<h2 id="sigmoid">Sigmoid</h2>

<p>Applies the element-wise function sigmoid(x) = 1 / ( 1 + exp(-x))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/sigmoid.png" ></p>

<h2 id="softmax">Softmax</h2>

<p>Applies the Softmax function to an n-dimensional input Tensor</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1</p>

<p>Softmax is defined as f_i(x) = exp(x_i - shift) / sum_j exp(x_j - shift)
                      where shift = max_i x_i</p>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * ]</td>
<td>2D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input with
    values in the range [0, 1]</p>

<p><img src="image/softmax.png" >
Notes:
    Note that this module doesn&rsquo;t work directly with NLLLoss,
    which expects the Log to be computed between the Softmax and itself.
    Use Logsoftmax instead (it&rsquo;s faster).</p>

<h2 id="softmax2d">Softmax2d</h2>

<p>Applies SoftMax over features to each spatial location</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax2d</span><span class="p">()</span>
<span class="c"># you softmax over the 2nd dimension</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>When given an image of Channels x Height x Width, it will
apply Softmax to each location [Channels, h_i, w_j]</p>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * , * , * ]</td>
<td>4D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input with
    values in the range [0, 1]</p>

<h2 id="softmin">Softmin</h2>

<p>Applies the Softmin function to an n-dimensional input Tensor</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmin</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>rescaling them so that the elements of the n-dimensional output Tensor
lie in the range (0,1) and sum to 1
Softmin(x) = exp(-x_i - shift) / sum_j exp(-x_j - shift)
             where shift = max_i - x_i</p>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>[ * , * ]</td>
<td>2D Tensor of any size</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input, with
    values in the range [0, 1]</p>

<p><img src="image/softmin.png" ></p>

<h2 id="softplus">Softplus</h2>

<p>Applies element-wise SoftPlus(x) = 1/beta * log(1 + exp(beta * x_i))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>SoftPlus is a smooth approximation to the ReLU function and can be used
to constrain the output of a machine to always be positive.
For numerical stability the implementation reverts to the linear function
for inputs above a certain value.</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>beta</td>
<td>1</td>
<td>the beta value for the Softplus formulation.</td>
</tr>
<tr>
<td>threshold</td>
<td>20</td>
<td>values above this revert to a linear function.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/softplus.png" ></p>

<h2 id="softshrink">Softshrink</h2>

<p>Applies the soft shrinkage function elementwise</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>SoftShrinkage operator is defined as:
    f(x) = x-lambda, if x &gt; lambda &gt;  f(x) = x+lambda, if x &lt; -lambda
    f(x) = 0, otherwise</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>lambd</td>
<td>0.5</td>
<td>the lambda value for the Softshrink formulation.</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/sshrink.png" ></p>

<h2 id="softsign">Softsign</h2>

<p>Applies element-wise, the function Softsign(x) = x / (1 + |x|)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/softsign.png" ></p>

<h2 id="tanh">Tanh</h2>

<p>Applies element-wise, Tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<p><img src="image/tanh.png" ></p>

<h2 id="tanhshrink">Tanhshrink</h2>

<p>Applies element-wise, Tanhshrink(x) = x - Tanh(x)</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanhshrink</span><span class="p">()</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>a Tensor of the same dimension and shape as the input</p>

<h2 id="threshold">Threshold</h2>

<p>Thresholds each element of the input Tensor</p>
<pre class="highlight python"><code><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Threshold</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
</code></pre>

<p>Threshold is defined as:
     y =  x        if x &gt;= threshold
          value    if x &lt;  threshold</p>

<h3 id="constructor-arguments">Constructor Arguments</h3>

<table><thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>threshold</td>
<td></td>
<td>The value to threshold at</td>
</tr>
<tr>
<td>value</td>
<td></td>
<td>The value to replace with</td>
</tr>
<tr>
<td>inplace</td>
<td></td>
<td>can optionally do the operation in-place</td>
</tr>
</tbody></table>

<h3 id="expected-shape">Expected Shape</h3>

<table><thead>
<tr>
<th></th>
<th>Shape</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>input</td>
<td>Any</td>
<td>Tensor of any size and dimension</td>
</tr>
<tr>
<td>output</td>
<td>Same</td>
<td>Output has the same shape as input</td>
</tr>
</tbody></table>

<h3 id="returns">Returns</h3>

<p>Tensor of same dimension and shape as the input</p>
